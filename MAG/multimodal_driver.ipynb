{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from typing import *\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, L1Loss, MSELoss\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from transformers import BertTokenizer, XLNetTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "from transformers.optimization import AdamW\n",
    "from bert import MAG_BertForSequenceClassification\n",
    "from xlnet import MAG_XLNetForSequenceClassification\n",
    "\n",
    "from argparse_utils import str2bool, seed\n",
    "from global_configs import ACOUSTIC_DIM, VISUAL_DIM, DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"dataset\": \"mosi\",\n",
    "    \"max_seq_length\": 50,\n",
    "    \"train_batch_size\": 48,\n",
    "    \"dev_batch_size\" : 128,\n",
    "    \"test_batch_size\": 128,\n",
    "    \"n_epochs\": 40,\n",
    "    \"beta_shift\": 1.0,\n",
    "    \"dropout_prob\": 0.5,\n",
    "    \"model\": \"bert-base-uncased\",\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"gradient_accumulation_step\": 1,\n",
    "    \"warmup_proportion\": 0.1,\n",
    "    \"seed\": seed(\"random\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, visual, acoustic, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.visual = visual\n",
    "        self.acoustic = acoustic\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalConfig(object):\n",
    "    def __init__(self, beta_shift, dropout_prob):\n",
    "        self.beta_shift = beta_shift\n",
    "        self.dropout_prob = dropout_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(examples, max_seq_length, tokenizer):\n",
    "    features = []\n",
    "\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "\n",
    "        (words, visual, acoustic), label_id, segment = example\n",
    "\n",
    "        tokens, inversions = [], []\n",
    "        for idx, word in enumerate(words):\n",
    "            tokenized = tokenizer.tokenize(word)\n",
    "            tokens.extend(tokenized)\n",
    "            inversions.extend([idx] * len(tokenized))\n",
    "\n",
    "        # Check inversion\n",
    "        assert len(tokens) == len(inversions)\n",
    "\n",
    "        aligned_visual = []\n",
    "        aligned_audio = []\n",
    "\n",
    "        for inv_idx in inversions:\n",
    "            aligned_visual.append(visual[inv_idx, :])\n",
    "            aligned_audio.append(acoustic[inv_idx, :])\n",
    "\n",
    "        visual = np.array(aligned_visual)\n",
    "        acoustic = np.array(aligned_audio)\n",
    "\n",
    "        # Truncate input if necessary\n",
    "        if len(tokens) > max_seq_length - 2:\n",
    "            tokens = tokens[: max_seq_length - 2]\n",
    "            acoustic = acoustic[: max_seq_length - 2]\n",
    "            visual = visual[: max_seq_length - 2]\n",
    "\n",
    "        if args.model == \"bert-base-uncased\":\n",
    "            prepare_input = prepare_bert_input\n",
    "        elif args.model == \"xlnet-base-cased\":\n",
    "            prepare_input = prepare_xlnet_input\n",
    "\n",
    "        input_ids, visual, acoustic, input_mask, segment_ids = prepare_input(\n",
    "            tokens, visual, acoustic, tokenizer\n",
    "        )\n",
    "\n",
    "        # Check input length\n",
    "        assert len(input_ids) == args.max_seq_length\n",
    "        assert len(input_mask) == args.max_seq_length\n",
    "        assert len(segment_ids) == args.max_seq_length\n",
    "        assert acoustic.shape[0] == args.max_seq_length\n",
    "        assert visual.shape[0] == args.max_seq_length\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                visual=visual,\n",
    "                acoustic=acoustic,\n",
    "                label_id=label_id,\n",
    "            )\n",
    "        )\n",
    "    return features\n",
    "\n",
    "\n",
    "def prepare_bert_input(tokens, visual, acoustic, tokenizer):\n",
    "    CLS = tokenizer.cls_token\n",
    "    SEP = tokenizer.sep_token\n",
    "    tokens = [CLS] + tokens + [SEP]\n",
    "\n",
    "    # Pad zero vectors for acoustic / visual vectors to account for [CLS] / [SEP] tokens\n",
    "    acoustic_zero = np.zeros((1, ACOUSTIC_DIM))\n",
    "    acoustic = np.concatenate((acoustic_zero, acoustic, acoustic_zero))\n",
    "    visual_zero = np.zeros((1, VISUAL_DIM))\n",
    "    visual = np.concatenate((visual_zero, visual, visual_zero))\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    segment_ids = [0] * len(input_ids)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    pad_length = args.max_seq_length - len(input_ids)\n",
    "\n",
    "    acoustic_padding = np.zeros((pad_length, ACOUSTIC_DIM))\n",
    "    acoustic = np.concatenate((acoustic, acoustic_padding))\n",
    "\n",
    "    visual_padding = np.zeros((pad_length, VISUAL_DIM))\n",
    "    visual = np.concatenate((visual, visual_padding))\n",
    "\n",
    "    padding = [0] * pad_length\n",
    "\n",
    "    # Pad inputs\n",
    "    input_ids += padding\n",
    "    input_mask += padding\n",
    "    segment_ids += padding\n",
    "\n",
    "    return input_ids, visual, acoustic, input_mask, segment_ids\n",
    "\n",
    "\n",
    "def prepare_xlnet_input(tokens, visual, acoustic, tokenizer):\n",
    "    CLS = tokenizer.cls_token\n",
    "    SEP = tokenizer.sep_token\n",
    "    PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "    # PAD special tokens\n",
    "    tokens = tokens + [SEP] + [CLS]\n",
    "    audio_zero = np.zeros((1, ACOUSTIC_DIM))\n",
    "    acoustic = np.concatenate((acoustic, audio_zero, audio_zero))\n",
    "    visual_zero = np.zeros((1, VISUAL_DIM))\n",
    "    visual = np.concatenate((visual, visual_zero, visual_zero))\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    segment_ids = [0] * (len(tokens) - 1) + [2]\n",
    "\n",
    "    pad_length = (args.max_seq_length - len(segment_ids))\n",
    "\n",
    "    # then zero pad the visual and acoustic\n",
    "    audio_padding = np.zeros((pad_length, ACOUSTIC_DIM))\n",
    "    acoustic = np.concatenate((audio_padding, acoustic))\n",
    "\n",
    "    video_padding = np.zeros((pad_length, VISUAL_DIM))\n",
    "    visual = np.concatenate((video_padding, visual))\n",
    "\n",
    "    input_ids = [PAD_ID] * pad_length + input_ids\n",
    "    input_mask = [0] * pad_length + input_mask\n",
    "    segment_ids = [3] * pad_length + segment_ids\n",
    "\n",
    "    return input_ids, visual, acoustic, input_mask, segment_ids\n",
    "\n",
    "\n",
    "def get_tokenizer(model):\n",
    "    if model == \"bert-base-uncased\":\n",
    "        return BertTokenizer.from_pretrained(model)\n",
    "    elif model == \"xlnet-base-cased\":\n",
    "        return XLNetTokenizer.from_pretrained(model)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Expected 'bert-base-uncased' or 'xlnet-base-cased, but received {}\".format(\n",
    "                model\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def get_appropriate_dataset(data):\n",
    "\n",
    "    tokenizer = get_tokenizer(args.model)\n",
    "\n",
    "    features = convert_to_features(data, args.max_seq_length, tokenizer)\n",
    "    all_input_ids = torch.tensor(\n",
    "        [f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor(\n",
    "        [f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor(\n",
    "        [f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_visual = torch.tensor([f.visual for f in features], dtype=torch.float)\n",
    "    all_acoustic = torch.tensor(\n",
    "        [f.acoustic for f in features], dtype=torch.float)\n",
    "    all_label_ids = torch.tensor(\n",
    "        [f.label_id for f in features], dtype=torch.float)\n",
    "\n",
    "    dataset = TensorDataset(\n",
    "        all_input_ids,\n",
    "        all_visual,\n",
    "        all_acoustic,\n",
    "        all_input_mask,\n",
    "        all_segment_ids,\n",
    "        all_label_ids,\n",
    "    )\n",
    "    return dataset, tokenizer\n",
    "\n",
    "\n",
    "def set_up_data_loader():\n",
    "    with open(f\"../datasets/{args.dataset}.pkl\", \"rb\") as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    train_data = data[\"train\"]\n",
    "    dev_data = data[\"dev\"]\n",
    "    test_data = data[\"test\"]\n",
    "\n",
    "    train_dataset, train_tokenizer = get_appropriate_dataset(train_data)\n",
    "    dev_dataset, dev_tokenizer = get_appropriate_dataset(dev_data)\n",
    "    test_dataset, test_tokenizer = get_appropriate_dataset(test_data)\n",
    "\n",
    "    num_train_optimization_steps = (\n",
    "        int(\n",
    "            len(train_dataset) / args.train_batch_size /\n",
    "            args.gradient_accumulation_step\n",
    "        )\n",
    "        * args.n_epochs\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=args.train_batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    dev_dataloader = DataLoader(\n",
    "        dev_dataset, batch_size=args.dev_batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=args.test_batch_size, shuffle=True,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        train_dataloader,\n",
    "        dev_dataloader,\n",
    "        test_dataloader,\n",
    "        num_train_optimization_steps,\n",
    "        train_tokenizer,\n",
    "        dev_tokenizer,\n",
    "        test_tokenizer\n",
    "    )\n",
    "\n",
    "\n",
    "def set_random_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function to seed experiment for reproducibility.\n",
    "    If -1 is provided as seed, experiment uses random seed from 0~9999\n",
    "\n",
    "    Args:\n",
    "        seed (int): integer to be used as seed, use -1 to randomly seed experiment\n",
    "    \"\"\"\n",
    "    print(\"Seed: {}\".format(seed))\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def prep_for_training(num_train_optimization_steps: int):\n",
    "    multimodal_config = MultimodalConfig(\n",
    "        beta_shift=args.beta_shift, dropout_prob=args.dropout_prob\n",
    "    )\n",
    "    bert_config = BertConfig(\n",
    "        hidden_dropout_prob=args.dropout_prob\n",
    "    )\n",
    "\n",
    "    if args.model == \"bert-base-uncased\":\n",
    "        model = MAG_BertForSequenceClassification.from_pretrained(\n",
    "            args.model, multimodal_config=multimodal_config, num_labels=1,\n",
    "        )\n",
    "        # model = BertForSequenceClassification.from_pretrained(\n",
    "        #     args.model,\n",
    "        #     num_labels = 1\n",
    "        # )\n",
    "    elif args.model == \"xlnet-base-cased\":\n",
    "        model = MAG_XLNetForSequenceClassification.from_pretrained(\n",
    "            args.model, multimodal_config=multimodal_config, num_labels=1\n",
    "        )\n",
    "\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Prepare optimizer\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_train_optimization_steps,\n",
    "        num_training_steps=args.warmup_proportion * num_train_optimization_steps,\n",
    "    )\n",
    "    return model, optimizer, scheduler\n",
    "\n",
    "\n",
    "def train_epoch(model: nn.Module, train_dataloader: DataLoader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "        batch = tuple(t.to(DEVICE) for t in batch)\n",
    "        input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "        visual = torch.squeeze(visual, 1)\n",
    "        acoustic = torch.squeeze(acoustic, 1)\n",
    "        model.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            visual,\n",
    "            acoustic,\n",
    "            token_type_ids=segment_ids,\n",
    "            attention_mask=input_mask,\n",
    "            labels=None\n",
    "        )\n",
    "\n",
    "        logits = outputs[0]\n",
    "        loss_fct = MSELoss()\n",
    "        loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "        if args.gradient_accumulation_step > 1:\n",
    "            loss = loss / args.gradient_accumulation_step\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        if (step + 1) % args.gradient_accumulation_step == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    return tr_loss / nb_tr_steps\n",
    "\n",
    "\n",
    "def eval_epoch(model: nn.Module, dev_dataloader: DataLoader, optimizer):\n",
    "    model.eval()\n",
    "    dev_loss = 0\n",
    "    nb_dev_examples, nb_dev_steps = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(tqdm(dev_dataloader, desc=\"Iteration\")):\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "            input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "            visual = torch.squeeze(visual, 1)\n",
    "            acoustic = torch.squeeze(acoustic, 1)\n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                visual,\n",
    "                acoustic,\n",
    "                token_type_ids=segment_ids,\n",
    "                attention_mask=input_mask,\n",
    "                labels=None\n",
    "            )\n",
    "            logits = outputs[0]\n",
    "\n",
    "            loss_fct = MSELoss()\n",
    "            loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "            if args.gradient_accumulation_step > 1:\n",
    "                loss = loss / args.gradient_accumulation_step\n",
    "\n",
    "            dev_loss += loss.item()\n",
    "            nb_dev_steps += 1\n",
    "\n",
    "    return dev_loss / nb_dev_steps\n",
    "\n",
    "\n",
    "def test_epoch(model: nn.Module, test_dataloader: DataLoader, tokenizer):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_dataloader)):\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "            input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "            visual = torch.squeeze(visual, 1)\n",
    "            acoustic = torch.squeeze(acoustic, 1)\n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                visual,\n",
    "                acoustic,\n",
    "                token_type_ids=segment_ids,\n",
    "                attention_mask=input_mask,\n",
    "                labels=None\n",
    "            )\n",
    "\n",
    "            logits = outputs[0]\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.detach().cpu().numpy()\n",
    "\n",
    "            logits = np.squeeze(logits).tolist()\n",
    "            label_ids = np.squeeze(label_ids).tolist()\n",
    "\n",
    "            preds.extend(logits)\n",
    "            labels.extend(label_ids)\n",
    "\n",
    "            # print(i, \" th batch\")\n",
    "            # for i, s in enumerate(input_ids):\n",
    "            #     tokens = tokenizer.convert_ids_to_tokens(s, skip_special_tokens = True)\n",
    "            #     print(tokens, logits[i], label_ids[i])\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # for s in input_ids:\n",
    "        #     tokens = tokenizer.convert_ids_to_tokens(s)\n",
    "        #     print(tokens)\n",
    "        # print(preds)\n",
    "        # print(labels)\n",
    "        # print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def test_score_model(model: nn.Module, test_dataloader: DataLoader, tokenizer, use_zero=False):\n",
    "\n",
    "    preds, y_test = test_epoch(model, test_dataloader, tokenizer)\n",
    "    non_zeros = np.array(\n",
    "        [i for i, e in enumerate(y_test) if e != 0 or use_zero])\n",
    "\n",
    "    preds = preds[non_zeros]\n",
    "    y_test = y_test[non_zeros]\n",
    "\n",
    "    mae = np.mean(np.absolute(preds - y_test))\n",
    "    corr = np.corrcoef(preds, y_test)[0][1]\n",
    "\n",
    "    preds = preds >= 0\n",
    "    y_test = y_test >= 0\n",
    "\n",
    "    f_score = f1_score(y_test, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "\n",
    "    return acc, mae, corr, f_score\n",
    "\n",
    "\n",
    "def test_instance(model: nn.Module, test_tokenizer):\n",
    "    model.eval()\n",
    "    segment_list = []\n",
    "    words_list = []\n",
    "    preds = []\n",
    "    preds_2 = []\n",
    "    preds_7 = []\n",
    "    labels = []\n",
    "    labels_2 = []\n",
    "    labels_7 = []\n",
    "\n",
    "    with open(f\"../datasets/{args.dataset}.pkl\", \"rb\") as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    # test_data[idx] = (words, visual, acoustic), label, segment\n",
    "    test_data = data[\"test\"]\n",
    "    test_dataset, test_tokenizer = get_appropriate_dataset(test_data)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=args.test_batch_size, shuffle=False,\n",
    "    )\n",
    "\n",
    "    video = set()\n",
    "    count = 0\n",
    "\n",
    "    for idx in range(len(test_data)):\n",
    "        (words, visual, acoustic), label, segment = test_data[idx]\n",
    "        if args.dataset == 'mosi':\n",
    "            segment_list.append(segment)\n",
    "        else:\n",
    "            video_name = segment[0]\n",
    "            if video_name in video:\n",
    "                count += 1\n",
    "            else:\n",
    "                video.add(video_name)\n",
    "                count = 0\n",
    "            segment_list.append(video_name + '[' + str(count) + ']')\n",
    "\n",
    "        words_list.append(words)\n",
    "        labels.append(label[0][0])\n",
    "\n",
    "        # label_2 appending\n",
    "        if label > 0:\n",
    "            labels_2.append('positive')\n",
    "        else:\n",
    "            labels_2.append('negative')\n",
    "        \n",
    "        # label_7 appending\n",
    "        if label < -15/7:\n",
    "            labels_7.append('very negative')\n",
    "        elif label < -9/7:\n",
    "            labels_7.append('negative')\n",
    "        elif label < -3/7:\n",
    "            labels_7.append('slightly negative')\n",
    "        elif label < 3/7:\n",
    "            labels_7.append('Neutral')\n",
    "        elif label < 9/7:\n",
    "            labels_7.append('slightly positive')\n",
    "        elif label < 15/7:\n",
    "            labels_7.append('positive')\n",
    "        else:\n",
    "            labels_7.append('very positive')\n",
    "            \n",
    "    # prediction\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_dataloader)):\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "            input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "            visual = torch.squeeze(visual, 1)\n",
    "            acoustic = torch.squeeze(acoustic, 1)\n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                visual,\n",
    "                acoustic,\n",
    "                token_type_ids=segment_ids,\n",
    "                attention_mask=input_mask,\n",
    "                labels=None\n",
    "            )\n",
    "            logits = outputs[0]\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.detach().cpu().numpy()\n",
    "\n",
    "            logits = np.squeeze(logits).tolist()\n",
    "            label_ids = np.squeeze(label_ids).tolist()\n",
    "\n",
    "            preds.extend(logits)\n",
    "\n",
    "            for logit in logits:\n",
    "                # preds_2 appending\n",
    "                if logit > 0:\n",
    "                    preds_2.append('positive')\n",
    "                else:\n",
    "                    preds_2.append('negative')\n",
    "\n",
    "                # label_7 appending\n",
    "                if logit < -15/7:\n",
    "                    preds_7.append('very negative')\n",
    "                elif logit < -9/7:\n",
    "                    preds_7.append('negative')\n",
    "                elif logit < -3/7:\n",
    "                    preds_7.append('slightly negative')\n",
    "                elif logit < 3/7:\n",
    "                    preds_7.append('Neutral')\n",
    "                elif logit < 9/7:\n",
    "                    preds_7.append('slightly positive')\n",
    "                elif logit < 15/7:\n",
    "                    preds_7.append('positive')\n",
    "                else:\n",
    "                    preds_7.append('very positive')\n",
    "\n",
    "            \n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(segment_list)):\n",
    "        print(i, \"th data\")\n",
    "        print(segment_list[i])\n",
    "        print(words_list[i])\n",
    "        print(labels[i])\n",
    "        print(labels_2[i])\n",
    "        print(labels_7[i])\n",
    "        print(preds[i])\n",
    "        print(preds_2[i])\n",
    "        print(preds_7[i])\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    validation_dataloader,\n",
    "    test_data_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    tokenizer\n",
    "):\n",
    "    valid_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch_i in range(int(args.n_epochs)):\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, scheduler)\n",
    "        valid_loss = eval_epoch(model, validation_dataloader, optimizer)\n",
    "        test_acc, test_mae, test_corr, test_f_score = test_score_model(\n",
    "            model, test_data_loader, tokenizer\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"epoch:{}, train_loss:{}, valid_loss:{}, test_acc:{}\".format(\n",
    "                epoch_i, train_loss, valid_loss, test_acc\n",
    "            )\n",
    "        )\n",
    "\n",
    "        valid_losses.append(valid_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "    \n",
    "    print(\"Total Result:\")\n",
    "    print(\"best_accuracy: \", sorted(test_accuracies)[-1])\n",
    "    print(\"best loss: \", sorted(valid_losses)[0])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 2530\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/soyeon/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:151: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_new.cpp:210.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MAG with beta_shift:1.0 hidden_prob:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing MAG_BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing MAG_BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MAG_BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MAG_BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.MAG.W_a.weight', 'bert.MAG.LayerNorm.weight', 'bert.MAG.W_v.weight', 'bert.MAG.W_v.bias', 'bert.MAG.W_hv.bias', 'bert.MAG.LayerNorm.bias', 'bert.MAG.W_ha.weight', 'bert.MAG.W_ha.bias', 'classifier.bias', 'bert.MAG.W_a.bias', 'bert.MAG.W_hv.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/mnt/soyeon/anaconda3/envs/pytorch/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "Iteration: 100%|██████████| 27/27 [00:03<00:00,  8.54it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 20.54it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:2.3206628296110363, valid_loss:2.7822484970092773, test_acc:0.4198473282442748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.77it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 20.57it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 20.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, train_loss:2.2976633133711637, valid_loss:2.7759132385253906, test_acc:0.4198473282442748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.56it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 20.24it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2, train_loss:2.289775667367158, valid_loss:2.7849926948547363, test_acc:0.4213740458015267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.58it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 20.31it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3, train_loss:2.286577264467875, valid_loss:2.7472140789031982, test_acc:0.4213740458015267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.68it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 20.42it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4, train_loss:2.2762422605797097, valid_loss:2.7410327196121216, test_acc:0.4213740458015267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.39it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 20.22it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5, train_loss:2.2380913849230164, valid_loss:2.69174861907959, test_acc:0.42290076335877863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.32it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 20.31it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6, train_loss:2.201779462673046, valid_loss:2.6395031213760376, test_acc:0.42442748091603055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.46it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 20.09it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7, train_loss:2.1346375898078636, valid_loss:2.6643381118774414, test_acc:0.42748091603053434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.11it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 20.22it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8, train_loss:2.0455225573645697, valid_loss:2.5028891563415527, test_acc:0.45648854961832064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00,  9.92it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 20.23it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9, train_loss:1.970657295650906, valid_loss:2.376025915145874, test_acc:0.4961832061068702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00,  9.73it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 20.06it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10, train_loss:1.8518164334473786, valid_loss:2.2400037050247192, test_acc:0.5572519083969466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00,  9.81it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.69it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11, train_loss:1.6677838299009535, valid_loss:1.9747759103775024, test_acc:0.6152671755725191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00,  9.85it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.75it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12, train_loss:1.509188179616575, valid_loss:1.6666821837425232, test_acc:0.7526717557251908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00,  9.85it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.66it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13, train_loss:1.4208437138133578, valid_loss:1.5167560577392578, test_acc:0.7725190839694657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00,  9.82it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 20.01it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14, train_loss:1.303062739195647, valid_loss:1.4173136353492737, test_acc:0.7908396946564885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00,  9.88it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 20.10it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15, train_loss:1.2421681262828685, valid_loss:1.3989813923835754, test_acc:0.7603053435114504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00,  9.99it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.47it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16, train_loss:1.2017787319642526, valid_loss:1.346500277519226, test_acc:0.7847328244274809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.08it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.56it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17, train_loss:1.10416841506958, valid_loss:1.251146674156189, test_acc:0.8122137404580153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.07it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 20.04it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 15.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18, train_loss:1.0039667310538116, valid_loss:1.250343680381775, test_acc:0.8106870229007633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00,  9.95it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.72it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19, train_loss:0.9354974914480139, valid_loss:1.250982403755188, test_acc:0.8106870229007633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.06it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.76it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20, train_loss:0.9043353398640951, valid_loss:1.2346599698066711, test_acc:0.816793893129771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.13it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.60it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21, train_loss:0.8621612897625676, valid_loss:1.2318817973136902, test_acc:0.8091603053435115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.04it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.58it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22, train_loss:0.8613306173571834, valid_loss:1.249707281589508, test_acc:0.8030534351145038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.12it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 20.06it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23, train_loss:0.7729315084439737, valid_loss:1.217372715473175, test_acc:0.8122137404580153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.10it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.69it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24, train_loss:0.7728768631264016, valid_loss:1.2711506485939026, test_acc:0.7984732824427481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.33it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 20.02it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25, train_loss:0.6941362630437922, valid_loss:1.2174089550971985, test_acc:0.8091603053435115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.38it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.59it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26, train_loss:0.668689481638096, valid_loss:1.2642387747764587, test_acc:0.7908396946564885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.29it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.59it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27, train_loss:0.6646923345548136, valid_loss:1.2015110850334167, test_acc:0.8137404580152672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.18it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.59it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28, train_loss:0.6053348558920401, valid_loss:1.259284645318985, test_acc:0.7984732824427481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.15it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.68it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29, train_loss:0.5730400361396648, valid_loss:1.2247944474220276, test_acc:0.8183206106870229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.14it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.59it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30, train_loss:0.5381483832995096, valid_loss:1.2177898287773132, test_acc:0.8076335877862595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.26it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.61it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31, train_loss:0.5250439445177714, valid_loss:1.2628591656684875, test_acc:0.8076335877862595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.10it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.61it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32, train_loss:0.453598032395045, valid_loss:1.263220489025116, test_acc:0.8198473282442749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.08it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.55it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33, train_loss:0.42017678198991, valid_loss:1.207796424627304, test_acc:0.7969465648854962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.08it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.66it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34, train_loss:0.3883508929499873, valid_loss:1.2557894587516785, test_acc:0.8091603053435115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.51it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.53it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35, train_loss:0.42076987911153724, valid_loss:1.197403907775879, test_acc:0.815267175572519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00,  9.98it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.57it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36, train_loss:0.3548144742294594, valid_loss:1.2059151530265808, test_acc:0.8274809160305343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.17it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.56it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37, train_loss:0.3272902496435024, valid_loss:1.2002331614494324, test_acc:0.8137404580152672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.18it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.48it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38, train_loss:0.3214496688710319, valid_loss:1.1988521218299866, test_acc:0.8259541984732824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 10.15it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 19.44it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 19.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39, train_loss:0.2986190655717143, valid_loss:1.1784008145332336, test_acc:0.8259541984732824\n",
      "Total Result:\n",
      "best_accuracy:  0.8274809160305343\n",
      "best loss:  1.1784008145332336\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    train_data_loader,\n",
    "    dev_data_loader,\n",
    "    test_data_loader,\n",
    "    num_train_optimization_steps,\n",
    "    train_tokenizer,\n",
    "    dev_tokenizer,\n",
    "    test_tokenizer\n",
    ") = set_up_data_loader()\n",
    "\n",
    "model, optimizer, scheduler = prep_for_training(\n",
    "    num_train_optimization_steps)\n",
    "\n",
    "model = train(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    dev_data_loader,\n",
    "    test_data_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    test_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./saved_models_MAG_mosi.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MAG with beta_shift:1.0 hidden_prob:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing MAG_BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing MAG_BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MAG_BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MAG_BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.MAG.W_a.weight', 'bert.MAG.LayerNorm.weight', 'bert.MAG.W_v.weight', 'bert.MAG.W_v.bias', 'bert.MAG.W_hv.bias', 'bert.MAG.LayerNorm.bias', 'bert.MAG.W_ha.weight', 'bert.MAG.W_ha.bias', 'classifier.bias', 'bert.MAG.W_a.bias', 'bert.MAG.W_hv.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MAG_BertForSequenceClassification(\n",
       "  (bert): MAG_BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (MAG): MAG(\n",
       "      (W_hv): Linear(in_features=815, out_features=768, bias=True)\n",
       "      (W_ha): Linear(in_features=842, out_features=768, bias=True)\n",
       "      (W_v): Linear(in_features=47, out_features=768, bias=True)\n",
       "      (W_a): Linear(in_features=74, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_config = MultimodalConfig(beta_shift=args.beta_shift, dropout_prob=args.dropout_prob)\n",
    "bert_config = BertConfig(hidden_dropout_prob=args.dropout_prob)\n",
    "model = MAG_BertForSequenceClassification.from_pretrained(args.model, multimodal_config=multimodal_config, num_labels=1)\n",
    "model.load_state_dict(torch.load(\"./saved_models_MAG_mosi.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Intensity Reflection of Fustion Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-93ae8bdca532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mpred_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mtest_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "with open(f\"../datasets/{args.dataset}.pkl\", \"rb\") as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "train_data = data[\"train\"]\n",
    "dev_data = data[\"dev\"]\n",
    "test_data = data[\"test\"]\n",
    "\n",
    "train_dataset, train_tokenizer = get_appropriate_dataset(train_data)\n",
    "dev_dataset, dev_tokenizer = get_appropriate_dataset(dev_data)\n",
    "test_dataset, test_tokenizer = get_appropriate_dataset(test_data)\n",
    "\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_embeddings = torch.zeros((0, 100), dtype=torch.float32)\n",
    "preds = []\n",
    "labels = []\n",
    "classes = []\n",
    "pred_classes = []\n",
    "\n",
    "# Gold 7-Class\n",
    "for idx in range(len(test_data)):\n",
    "    (word, visual, acoustic), label, segment = test_data[idx]\n",
    "    if label < -15/7:\n",
    "        classes.append(-3)\n",
    "    elif label < -9/7:\n",
    "        classes.append(-2)\n",
    "    elif label < -3/7:\n",
    "        classes.append(-1)\n",
    "    elif label < 3/7:\n",
    "        classes.append(0)\n",
    "    elif label < 9/7:\n",
    "        classes.append(1)\n",
    "    elif label < 15/7:\n",
    "        classes.append(2)\n",
    "    else:\n",
    "        classes.append(3)\n",
    "classes = np.array(classes)\n",
    "\n",
    "# MAG-BERT Model output\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(test_data_loader)):\n",
    "        batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "        input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "        visual = torch.squeeze(visual, 1)\n",
    "        acoustic = torch.squeeze(acoustic, 1)\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            visual,\n",
    "            acoustic,\n",
    "            token_type_ids=segment_ids,\n",
    "            attention_mask=input_mask,\n",
    "            labels=None\n",
    "        )\n",
    "\n",
    "        logits = outputs[0]\n",
    "        embeddings = outputs[1:]\n",
    "\n",
    "        test_embeddings = torch.cat((test_embeddings, embeddings.detach().cpu()), 0)\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.detach().cpu().numpy()\n",
    "\n",
    "        preds.extend(np.squeeze(logits).tolist())\n",
    "        labels.extend(np.squeeze(label_ids).tolist())\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # 7-class prediction\n",
    "        for logit in logits:\n",
    "            if logit < -15/7:\n",
    "                pred_classes.append(-3)\n",
    "            elif logit < -9/7:\n",
    "                pred_classes.append(-2)\n",
    "            elif logit < -3/7:\n",
    "                pred_classes.append(-1)\n",
    "            elif logit < 3/7:\n",
    "                pred_classes.append(0)\n",
    "            elif logit < 9/7:\n",
    "                pred_classes.append(1)\n",
    "            elif logit < 15/7:\n",
    "                pred_classes.append(2)\n",
    "            else:\n",
    "                pred_classes.append(3)\n",
    "        pred_classes = np.array(pred_classes)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a two dimensional t-SNE projection of the embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(2, verbose=1)\n",
    "tsne_proj = tsne.fit_transform(test_embeddings)\n",
    "cmap = cm.get_cmap('tab20')\n",
    "fig, ax = plt.subplot(figsize=(8,8))\n",
    "# num_categories = 7\n",
    "for lab in range(-3, 3):\n",
    "    indices = pred_classes==lab\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Instance Prediction Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_instance(model: nn.Module):\n",
    "    model.eval()\n",
    "    segment_list = []\n",
    "    words_list = []\n",
    "    preds = []\n",
    "    preds_2 = []\n",
    "    preds_7 = []\n",
    "    labels = []\n",
    "    labels_2 = []\n",
    "    labels_7 = []\n",
    "\n",
    "    with open(f\"../datasets/{args.dataset}.pkl\", \"rb\") as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    # test_data[idx] = (words, visual, acoustic), label, segment\n",
    "    test_data = data[\"test\"]\n",
    "    test_dataset, test_tokenizer = get_appropriate_dataset(test_data)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=args.test_batch_size, shuffle=False,\n",
    "    )\n",
    "\n",
    "    video = set()\n",
    "    count = 0\n",
    "\n",
    "    for idx in range(len(test_data)):\n",
    "        (words, visual, acoustic), label, segment = test_data[idx]\n",
    "        if args.dataset == 'mosi':\n",
    "            segment_list.append(segment)\n",
    "        else:\n",
    "            video_name = segment[0]\n",
    "            if video_name in video:\n",
    "                count += 1\n",
    "            else:\n",
    "                video.add(video_name)\n",
    "                count = 0\n",
    "            segment_list.append(video_name + '[' + str(count) + ']')\n",
    "\n",
    "        words_list.append(words)\n",
    "        labels.append(label[0][0])\n",
    "\n",
    "        # label_2 appending\n",
    "        if label > 0:\n",
    "            labels_2.append('positive')\n",
    "        else:\n",
    "            labels_2.append('negative')\n",
    "        \n",
    "        # label_7 appending\n",
    "        if label < -15/7:\n",
    "            labels_7.append('very negative')\n",
    "        elif label < -9/7:\n",
    "            labels_7.append('negative')\n",
    "        elif label < -3/7:\n",
    "            labels_7.append('slightly negative')\n",
    "        elif label < 3/7:\n",
    "            labels_7.append('Neutral')\n",
    "        elif label < 9/7:\n",
    "            labels_7.append('slightly positive')\n",
    "        elif label < 15/7:\n",
    "            labels_7.append('positive')\n",
    "        else:\n",
    "            labels_7.append('very positive')\n",
    "            \n",
    "    # prediction\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_dataloader)):\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "            input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "            visual = torch.squeeze(visual, 1)\n",
    "            acoustic = torch.squeeze(acoustic, 1)\n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                visual,\n",
    "                acoustic,\n",
    "                token_type_ids=segment_ids,\n",
    "                attention_mask=input_mask,\n",
    "                labels=None\n",
    "            )\n",
    "            logits = outputs[0]\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.detach().cpu().numpy()\n",
    "\n",
    "            logits = np.squeeze(logits).tolist()\n",
    "            label_ids = np.squeeze(label_ids).tolist()\n",
    "\n",
    "            preds.extend(logits)\n",
    "\n",
    "            for logit in logits:\n",
    "                # preds_2 appending\n",
    "                if logit > 0:\n",
    "                    preds_2.append('positive')\n",
    "                else:\n",
    "                    preds_2.append('negative')\n",
    "\n",
    "                # label_7 appending\n",
    "                if logit < -15/7:\n",
    "                    preds_7.append('very negative')\n",
    "                elif logit < -9/7:\n",
    "                    preds_7.append('negative')\n",
    "                elif logit < -3/7:\n",
    "                    preds_7.append('slightly negative')\n",
    "                elif logit < 3/7:\n",
    "                    preds_7.append('Neutral')\n",
    "                elif logit < 9/7:\n",
    "                    preds_7.append('slightly positive')\n",
    "                elif logit < 15/7:\n",
    "                    preds_7.append('positive')\n",
    "                else:\n",
    "                    preds_7.append('very positive')\n",
    "\n",
    "                # if logit < -15/7:\n",
    "                #     preds_7.append('-3')\n",
    "                # elif logit < -9/7:\n",
    "                #     preds_7.append('-2')\n",
    "                # elif logit < -3/7:\n",
    "                #     preds_7.append('-1')\n",
    "                # elif logit < 3/7:\n",
    "                #     preds_7.append('0')\n",
    "                # elif logit < 9/7:\n",
    "                #     preds_7.append('1')\n",
    "                # elif logit < 15/7:\n",
    "                #     preds_7.append('2')\n",
    "                # else:\n",
    "                #     preds_7.append('3')\n",
    "\n",
    "            \n",
    "\n",
    "    count = 0\n",
    "    # for i in range(len(segment_list)):\n",
    "    #     print(i, \"th data\")\n",
    "    #     print(segment_list[i])\n",
    "    #     print(words_list[i])\n",
    "    #     print(labels[i])\n",
    "    #     print(labels_2[i])\n",
    "    #     print(labels_7[i])\n",
    "    #     print(preds[i])\n",
    "    #     print(preds_2[i])\n",
    "    #     print(preds_7[i])\n",
    "\n",
    "    return segment_list, words_list, labels, labels_2, labels_7, preds, preds_2, preds_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 20.08it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.to(DEVICE)\n",
    "segment_list, words_list, labels, labels_2, labels_7, preds, preds_2, preds_7 = test_instance(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2cfcba5631346c7ae72ba326195b606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='idx', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def get_predict_result(idx = range(len(segment_list))):\n",
    "    print(\"SEGMENT:\", segment_list[idx])\n",
    "    print(\"WORDS:\", words_list[idx])\n",
    "    print(\"GOLD_VALUE:\", labels[idx])\n",
    "    print(\"GOLD_BINARY:\", labels_2[idx])\n",
    "    print(\"GOLD_7_CLASS:\", labels_7[idx])\n",
    "    print(\"PREDICTED_VALUE:\", preds[idx])\n",
    "    print(\"PREDICTED_BINARY:\", preds_2[idx])\n",
    "    print(\"PREDICTED _7_CLASS:\", preds_7[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "d = {'segmentID': segment_list, 'words': words_list, 'labels': labels, 'labels_2': labels_2, 'labels_7': labels_7, 'preds': preds, 'preds_2': preds_2, 'preds_7': preds_7}\n",
    "df = pd.DataFrame(data=d)\n",
    "order = ['very negative', 'negative', 'slightly negative', 'Neutral', 'slightly positive', 'positive', 'very positive']\n",
    "p = sns.displot(df, x=\"preds_7\", hue_order=order)\n",
    "p.fig.set_dpi(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "658084397260d4aab479b7df4163d6eb096562a13a5b5f6c649e3c1d5e34b6c2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
