{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from typing import *\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, L1Loss, MSELoss\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from transformers import BertTokenizer, XLNetTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "from transformers.optimization import AdamW\n",
    "from bert import MAG_BertForSequenceClassification\n",
    "from xlnet import MAG_XLNetForSequenceClassification\n",
    "\n",
    "from argparse_utils import str2bool, seed\n",
    "from global_configs import ACOUSTIC_DIM, VISUAL_DIM, DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"dataset\": \"mosi\",\n",
    "    \"max_seq_length\": 50,\n",
    "    \"train_batch_size\": 48,\n",
    "    \"dev_batch_size\" : 128,\n",
    "    \"test_batch_size\": 128,\n",
    "    \"n_epochs\": 40,\n",
    "    \"beta_shift\": 1.0,\n",
    "    \"dropout_prob\": 0.5,\n",
    "    \"model\": \"bert-base-uncased\",\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"gradient_accumulation_step\": 1,\n",
    "    \"warmup_proportion\": 0.1,\n",
    "    \"seed\": seed(\"random\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, visual, acoustic, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.visual = visual\n",
    "        self.acoustic = acoustic\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalConfig(object):\n",
    "    def __init__(self, beta_shift, dropout_prob):\n",
    "        self.beta_shift = beta_shift\n",
    "        self.dropout_prob = dropout_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(examples, max_seq_length, tokenizer):\n",
    "    features = []\n",
    "\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "\n",
    "        (words, visual, acoustic), label_id, segment = example\n",
    "\n",
    "        tokens, inversions = [], []\n",
    "        for idx, word in enumerate(words):\n",
    "            tokenized = tokenizer.tokenize(word)\n",
    "            tokens.extend(tokenized)\n",
    "            inversions.extend([idx] * len(tokenized))\n",
    "\n",
    "        # Check inversion\n",
    "        assert len(tokens) == len(inversions)\n",
    "\n",
    "        aligned_visual = []\n",
    "        aligned_audio = []\n",
    "\n",
    "        for inv_idx in inversions:\n",
    "            aligned_visual.append(visual[inv_idx, :])\n",
    "            aligned_audio.append(acoustic[inv_idx, :])\n",
    "\n",
    "        visual = np.array(aligned_visual)\n",
    "        acoustic = np.array(aligned_audio)\n",
    "\n",
    "        # Truncate input if necessary\n",
    "        if len(tokens) > max_seq_length - 2:\n",
    "            tokens = tokens[: max_seq_length - 2]\n",
    "            acoustic = acoustic[: max_seq_length - 2]\n",
    "            visual = visual[: max_seq_length - 2]\n",
    "\n",
    "        if args.model == \"bert-base-uncased\":\n",
    "            prepare_input = prepare_bert_input\n",
    "        elif args.model == \"xlnet-base-cased\":\n",
    "            prepare_input = prepare_xlnet_input\n",
    "\n",
    "        input_ids, visual, acoustic, input_mask, segment_ids = prepare_input(\n",
    "            tokens, visual, acoustic, tokenizer\n",
    "        )\n",
    "\n",
    "        # Check input length\n",
    "        assert len(input_ids) == args.max_seq_length\n",
    "        assert len(input_mask) == args.max_seq_length\n",
    "        assert len(segment_ids) == args.max_seq_length\n",
    "        assert acoustic.shape[0] == args.max_seq_length\n",
    "        assert visual.shape[0] == args.max_seq_length\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                visual=visual,\n",
    "                acoustic=acoustic,\n",
    "                label_id=label_id,\n",
    "            )\n",
    "        )\n",
    "    return features\n",
    "\n",
    "\n",
    "def prepare_bert_input(tokens, visual, acoustic, tokenizer):\n",
    "    CLS = tokenizer.cls_token\n",
    "    SEP = tokenizer.sep_token\n",
    "    tokens = [CLS] + tokens + [SEP]\n",
    "\n",
    "    # Pad zero vectors for acoustic / visual vectors to account for [CLS] / [SEP] tokens\n",
    "    acoustic_zero = np.zeros((1, ACOUSTIC_DIM))\n",
    "    acoustic = np.concatenate((acoustic_zero, acoustic, acoustic_zero))\n",
    "    visual_zero = np.zeros((1, VISUAL_DIM))\n",
    "    visual = np.concatenate((visual_zero, visual, visual_zero))\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    segment_ids = [0] * len(input_ids)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    pad_length = args.max_seq_length - len(input_ids)\n",
    "\n",
    "    acoustic_padding = np.zeros((pad_length, ACOUSTIC_DIM))\n",
    "    acoustic = np.concatenate((acoustic, acoustic_padding))\n",
    "\n",
    "    visual_padding = np.zeros((pad_length, VISUAL_DIM))\n",
    "    visual = np.concatenate((visual, visual_padding))\n",
    "\n",
    "    padding = [0] * pad_length\n",
    "\n",
    "    # Pad inputs\n",
    "    input_ids += padding\n",
    "    input_mask += padding\n",
    "    segment_ids += padding\n",
    "\n",
    "    return input_ids, visual, acoustic, input_mask, segment_ids\n",
    "\n",
    "\n",
    "def prepare_xlnet_input(tokens, visual, acoustic, tokenizer):\n",
    "    CLS = tokenizer.cls_token\n",
    "    SEP = tokenizer.sep_token\n",
    "    PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "    # PAD special tokens\n",
    "    tokens = tokens + [SEP] + [CLS]\n",
    "    audio_zero = np.zeros((1, ACOUSTIC_DIM))\n",
    "    acoustic = np.concatenate((acoustic, audio_zero, audio_zero))\n",
    "    visual_zero = np.zeros((1, VISUAL_DIM))\n",
    "    visual = np.concatenate((visual, visual_zero, visual_zero))\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    segment_ids = [0] * (len(tokens) - 1) + [2]\n",
    "\n",
    "    pad_length = (args.max_seq_length - len(segment_ids))\n",
    "\n",
    "    # then zero pad the visual and acoustic\n",
    "    audio_padding = np.zeros((pad_length, ACOUSTIC_DIM))\n",
    "    acoustic = np.concatenate((audio_padding, acoustic))\n",
    "\n",
    "    video_padding = np.zeros((pad_length, VISUAL_DIM))\n",
    "    visual = np.concatenate((video_padding, visual))\n",
    "\n",
    "    input_ids = [PAD_ID] * pad_length + input_ids\n",
    "    input_mask = [0] * pad_length + input_mask\n",
    "    segment_ids = [3] * pad_length + segment_ids\n",
    "\n",
    "    return input_ids, visual, acoustic, input_mask, segment_ids\n",
    "\n",
    "\n",
    "def get_tokenizer(model):\n",
    "    if model == \"bert-base-uncased\":\n",
    "        return BertTokenizer.from_pretrained(model)\n",
    "    elif model == \"xlnet-base-cased\":\n",
    "        return XLNetTokenizer.from_pretrained(model)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Expected 'bert-base-uncased' or 'xlnet-base-cased, but received {}\".format(\n",
    "                model\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def get_appropriate_dataset(data):\n",
    "\n",
    "    tokenizer = get_tokenizer(args.model)\n",
    "\n",
    "    features = convert_to_features(data, args.max_seq_length, tokenizer)\n",
    "    all_input_ids = torch.tensor(\n",
    "        [f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor(\n",
    "        [f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor(\n",
    "        [f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_visual = torch.tensor([f.visual for f in features], dtype=torch.float)\n",
    "    all_acoustic = torch.tensor(\n",
    "        [f.acoustic for f in features], dtype=torch.float)\n",
    "    all_label_ids = torch.tensor(\n",
    "        [f.label_id for f in features], dtype=torch.float)\n",
    "\n",
    "    dataset = TensorDataset(\n",
    "        all_input_ids,\n",
    "        all_visual,\n",
    "        all_acoustic,\n",
    "        all_input_mask,\n",
    "        all_segment_ids,\n",
    "        all_label_ids,\n",
    "    )\n",
    "    return dataset, tokenizer\n",
    "\n",
    "\n",
    "def set_up_data_loader():\n",
    "    with open(f\"../datasets/{args.dataset}.pkl\", \"rb\") as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    train_data = data[\"train\"]\n",
    "    dev_data = data[\"dev\"]\n",
    "    test_data = data[\"test\"]\n",
    "\n",
    "    train_dataset, train_tokenizer = get_appropriate_dataset(train_data)\n",
    "    dev_dataset, dev_tokenizer = get_appropriate_dataset(dev_data)\n",
    "    test_dataset, test_tokenizer = get_appropriate_dataset(test_data)\n",
    "\n",
    "    num_train_optimization_steps = (\n",
    "        int(\n",
    "            len(train_dataset) / args.train_batch_size /\n",
    "            args.gradient_accumulation_step\n",
    "        )\n",
    "        * args.n_epochs\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=args.train_batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    dev_dataloader = DataLoader(\n",
    "        dev_dataset, batch_size=args.dev_batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=args.test_batch_size, shuffle=True,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        train_dataloader,\n",
    "        dev_dataloader,\n",
    "        test_dataloader,\n",
    "        num_train_optimization_steps,\n",
    "        train_tokenizer,\n",
    "        dev_tokenizer,\n",
    "        test_tokenizer\n",
    "    )\n",
    "\n",
    "\n",
    "def set_random_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function to seed experiment for reproducibility.\n",
    "    If -1 is provided as seed, experiment uses random seed from 0~9999\n",
    "\n",
    "    Args:\n",
    "        seed (int): integer to be used as seed, use -1 to randomly seed experiment\n",
    "    \"\"\"\n",
    "    print(\"Seed: {}\".format(seed))\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def prep_for_training(num_train_optimization_steps: int):\n",
    "    multimodal_config = MultimodalConfig(\n",
    "        beta_shift=args.beta_shift, dropout_prob=args.dropout_prob\n",
    "    )\n",
    "    bert_config = BertConfig(\n",
    "        hidden_dropout_prob=args.dropout_prob\n",
    "    )\n",
    "\n",
    "    if args.model == \"bert-base-uncased\":\n",
    "        model = MAG_BertForSequenceClassification.from_pretrained(\n",
    "            args.model, multimodal_config=multimodal_config, num_labels=1,\n",
    "        )\n",
    "        # model = BertForSequenceClassification.from_pretrained(\n",
    "        #     args.model,\n",
    "        #     num_labels = 1\n",
    "        # )\n",
    "    elif args.model == \"xlnet-base-cased\":\n",
    "        model = MAG_XLNetForSequenceClassification.from_pretrained(\n",
    "            args.model, multimodal_config=multimodal_config, num_labels=1\n",
    "        )\n",
    "\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Prepare optimizer\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_train_optimization_steps,\n",
    "        num_training_steps=args.warmup_proportion * num_train_optimization_steps,\n",
    "    )\n",
    "    return model, optimizer, scheduler\n",
    "\n",
    "\n",
    "def train_epoch(model: nn.Module, train_dataloader: DataLoader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "        batch = tuple(t.to(DEVICE) for t in batch)\n",
    "        input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "        visual = torch.squeeze(visual, 1)\n",
    "        acoustic = torch.squeeze(acoustic, 1)\n",
    "        model.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            visual,\n",
    "            acoustic,\n",
    "            token_type_ids=segment_ids,\n",
    "            attention_mask=input_mask,\n",
    "            labels=None\n",
    "        )\n",
    "\n",
    "        logits = outputs[0]\n",
    "        loss_fct = MSELoss()\n",
    "        loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "        if args.gradient_accumulation_step > 1:\n",
    "            loss = loss / args.gradient_accumulation_step\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        if (step + 1) % args.gradient_accumulation_step == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    return tr_loss / nb_tr_steps\n",
    "\n",
    "\n",
    "def eval_epoch(model: nn.Module, dev_dataloader: DataLoader, optimizer):\n",
    "    model.eval()\n",
    "    dev_loss = 0\n",
    "    nb_dev_examples, nb_dev_steps = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(tqdm(dev_dataloader, desc=\"Iteration\")):\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "            input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "            visual = torch.squeeze(visual, 1)\n",
    "            acoustic = torch.squeeze(acoustic, 1)\n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                visual,\n",
    "                acoustic,\n",
    "                token_type_ids=segment_ids,\n",
    "                attention_mask=input_mask,\n",
    "                labels=None\n",
    "            )\n",
    "            logits = outputs[0]\n",
    "\n",
    "            loss_fct = MSELoss()\n",
    "            loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "            if args.gradient_accumulation_step > 1:\n",
    "                loss = loss / args.gradient_accumulation_step\n",
    "\n",
    "            dev_loss += loss.item()\n",
    "            nb_dev_steps += 1\n",
    "\n",
    "    return dev_loss / nb_dev_steps\n",
    "\n",
    "\n",
    "def test_epoch(model: nn.Module, test_dataloader: DataLoader, tokenizer):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_dataloader)):\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "            input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "            visual = torch.squeeze(visual, 1)\n",
    "            acoustic = torch.squeeze(acoustic, 1)\n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                visual,\n",
    "                acoustic,\n",
    "                token_type_ids=segment_ids,\n",
    "                attention_mask=input_mask,\n",
    "                labels=None\n",
    "            )\n",
    "\n",
    "            logits = outputs[0]\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.detach().cpu().numpy()\n",
    "\n",
    "            logits = np.squeeze(logits).tolist()\n",
    "            label_ids = np.squeeze(label_ids).tolist()\n",
    "\n",
    "            preds.extend(logits)\n",
    "            labels.extend(label_ids)\n",
    "\n",
    "            # print(i, \" th batch\")\n",
    "            # for i, s in enumerate(input_ids):\n",
    "            #     tokens = tokenizer.convert_ids_to_tokens(s, skip_special_tokens = True)\n",
    "            #     print(tokens, logits[i], label_ids[i])\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # for s in input_ids:\n",
    "        #     tokens = tokenizer.convert_ids_to_tokens(s)\n",
    "        #     print(tokens)\n",
    "        # print(preds)\n",
    "        # print(labels)\n",
    "        # print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def test_score_model(model: nn.Module, test_dataloader: DataLoader, tokenizer, use_zero=False):\n",
    "\n",
    "    preds, y_test = test_epoch(model, test_dataloader, tokenizer)\n",
    "    non_zeros = np.array(\n",
    "        [i for i, e in enumerate(y_test) if e != 0 or use_zero])\n",
    "\n",
    "    preds = preds[non_zeros]\n",
    "    y_test = y_test[non_zeros]\n",
    "\n",
    "    mae = np.mean(np.absolute(preds - y_test))\n",
    "    corr = np.corrcoef(preds, y_test)[0][1]\n",
    "\n",
    "    preds = preds >= 0\n",
    "    y_test = y_test >= 0\n",
    "\n",
    "    f_score = f1_score(y_test, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "\n",
    "    return acc, mae, corr, f_score\n",
    "\n",
    "\n",
    "def test_instance(model: nn.Module, test_tokenizer):\n",
    "    model.eval()\n",
    "    segment_list = []\n",
    "    words_list = []\n",
    "    preds = []\n",
    "    preds_2 = []\n",
    "    preds_7 = []\n",
    "    labels = []\n",
    "    labels_2 = []\n",
    "    labels_7 = []\n",
    "\n",
    "    with open(f\"../datasets/{args.dataset}.pkl\", \"rb\") as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    # test_data[idx] = (words, visual, acoustic), label, segment\n",
    "    test_data = data[\"test\"]\n",
    "    test_dataset, test_tokenizer = get_appropriate_dataset(test_data)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=args.test_batch_size, shuffle=False,\n",
    "    )\n",
    "\n",
    "    video = set()\n",
    "    count = 0\n",
    "\n",
    "    for idx in range(len(test_data)):\n",
    "        (words, visual, acoustic), label, segment = test_data[idx]\n",
    "        if args.dataset == 'mosi':\n",
    "            segment_list.append(segment)\n",
    "        else:\n",
    "            video_name = segment[0]\n",
    "            if video_name in video:\n",
    "                count += 1\n",
    "            else:\n",
    "                video.add(video_name)\n",
    "                count = 0\n",
    "            segment_list.append(video_name + '[' + str(count) + ']')\n",
    "\n",
    "        words_list.append(words)\n",
    "        labels.append(label[0][0])\n",
    "\n",
    "        # label_2 appending\n",
    "        if label > 0:\n",
    "            labels_2.append('positive')\n",
    "        else:\n",
    "            labels_2.append('negative')\n",
    "        \n",
    "        # label_7 appending\n",
    "        if label < -15/7:\n",
    "            labels_7.append('very negative')\n",
    "        elif label < -9/7:\n",
    "            labels_7.append('negative')\n",
    "        elif label < -3/7:\n",
    "            labels_7.append('slightly negative')\n",
    "        elif label < 3/7:\n",
    "            labels_7.append('Neutral')\n",
    "        elif label < 9/7:\n",
    "            labels_7.append('slightly positive')\n",
    "        elif label < 15/7:\n",
    "            labels_7.append('positive')\n",
    "        else:\n",
    "            labels_7.append('very positive')\n",
    "            \n",
    "    # prediction\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_dataloader)):\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "            input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "            visual = torch.squeeze(visual, 1)\n",
    "            acoustic = torch.squeeze(acoustic, 1)\n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                visual,\n",
    "                acoustic,\n",
    "                token_type_ids=segment_ids,\n",
    "                attention_mask=input_mask,\n",
    "                labels=None\n",
    "            )\n",
    "            logits = outputs[0]\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.detach().cpu().numpy()\n",
    "\n",
    "            logits = np.squeeze(logits).tolist()\n",
    "            label_ids = np.squeeze(label_ids).tolist()\n",
    "\n",
    "            preds.extend(logits)\n",
    "\n",
    "            for logit in logits:\n",
    "                # preds_2 appending\n",
    "                if logit > 0:\n",
    "                    preds_2.append('positive')\n",
    "                else:\n",
    "                    preds_2.append('negative')\n",
    "\n",
    "                # label_7 appending\n",
    "                if logit < -15/7:\n",
    "                    preds_7.append('very negative')\n",
    "                elif logit < -9/7:\n",
    "                    preds_7.append('negative')\n",
    "                elif logit < -3/7:\n",
    "                    preds_7.append('slightly negative')\n",
    "                elif logit < 3/7:\n",
    "                    preds_7.append('Neutral')\n",
    "                elif logit < 9/7:\n",
    "                    preds_7.append('slightly positive')\n",
    "                elif logit < 15/7:\n",
    "                    preds_7.append('positive')\n",
    "                else:\n",
    "                    preds_7.append('very positive')\n",
    "\n",
    "            \n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(segment_list)):\n",
    "        print(i, \"th data\")\n",
    "        print(segment_list[i])\n",
    "        print(words_list[i])\n",
    "        print(labels[i])\n",
    "        print(labels_2[i])\n",
    "        print(labels_7[i])\n",
    "        print(preds[i])\n",
    "        print(preds_2[i])\n",
    "        print(preds_7[i])\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    validation_dataloader,\n",
    "    test_data_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    tokenizer\n",
    "):\n",
    "    valid_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch_i in range(int(args.n_epochs)):\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, scheduler)\n",
    "        valid_loss = eval_epoch(model, validation_dataloader, optimizer)\n",
    "        test_acc, test_mae, test_corr, test_f_score = test_score_model(\n",
    "            model, test_data_loader, tokenizer\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"epoch:{}, train_loss:{}, valid_loss:{}, test_acc:{}\".format(\n",
    "                epoch_i, train_loss, valid_loss, test_acc\n",
    "            )\n",
    "        )\n",
    "\n",
    "        valid_losses.append(valid_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "    \n",
    "    print(\"Total Result:\")\n",
    "    print(\"best_accuracy: \", sorted(test_accuracies)[-1])\n",
    "    print(\"best loss: \", sorted(valid_losses)[0])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 2949\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MAG with beta_shift:1.0 hidden_prob:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing MAG_BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing MAG_BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MAG_BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MAG_BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.MAG.W_a.bias', 'bert.MAG.W_a.weight', 'bert.MAG.LayerNorm.weight', 'bert.MAG.W_hv.weight', 'bert.MAG.W_ha.weight', 'bert.MAG.W_hv.bias', 'classifier.weight', 'classifier.bias', 'bert.MAG.LayerNorm.bias', 'bert.MAG.W_ha.bias', 'bert.MAG.W_v.bias', 'bert.MAG.W_v.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ubuntu/anaconda3/envs/pytorch1.7.1_p37/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.34it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.06it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.27it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:2.312387563564159, valid_loss:2.760565757751465, test_acc:0.47022900763358777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.88it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 35.38it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 35.04it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 13.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, train_loss:2.2905799636134394, valid_loss:2.7379735708236694, test_acc:0.46564885496183206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.92it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 35.01it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.83it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2, train_loss:2.2826085267243563, valid_loss:2.7523261308670044, test_acc:0.45648854961832064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.26it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.98it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.59it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:02, 12.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3, train_loss:2.297285887930128, valid_loss:2.722131133079529, test_acc:0.4549618320610687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.72it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 33.11it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.74it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4, train_loss:2.274548711600127, valid_loss:2.707979202270508, test_acc:0.46412213740458014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.00it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 35.28it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 33.91it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:02, 11.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5, train_loss:2.252543149171052, valid_loss:2.6691495180130005, test_acc:0.48854961832061067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.41it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.95it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.54it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6, train_loss:2.248262634983769, valid_loss:2.6556795835494995, test_acc:0.549618320610687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.75it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 35.07it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.47it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:02, 12.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7, train_loss:2.185562844629641, valid_loss:2.570175886154175, test_acc:0.5923664122137404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.71it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.24it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.78it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8, train_loss:2.153342498673333, valid_loss:2.5185261964797974, test_acc:0.5893129770992367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.99it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.71it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.75it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9, train_loss:2.0808859533733792, valid_loss:2.405665636062622, test_acc:0.5923664122137404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.04it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.66it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.77it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10, train_loss:1.9690306407433968, valid_loss:2.2676409482955933, test_acc:0.5984732824427481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.04it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.95it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.72it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11, train_loss:1.8319963658297505, valid_loss:2.054684817790985, test_acc:0.6274809160305344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.05it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.62it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.57it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12, train_loss:1.6378055987534699, valid_loss:1.764476716518402, test_acc:0.7251908396946565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.05it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.77it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.70it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13, train_loss:1.4116337255195335, valid_loss:1.523555040359497, test_acc:0.7954198473282442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.06it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.70it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.68it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14, train_loss:1.2695813046561346, valid_loss:1.4019355177879333, test_acc:0.815267175572519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.96it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.73it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.43it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:02, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15, train_loss:1.269033913259153, valid_loss:1.3111392259597778, test_acc:0.8183206106870229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.91it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.67it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.66it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16, train_loss:1.1556970521255776, valid_loss:1.2672472596168518, test_acc:0.8213740458015267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.04it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.76it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.64it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17, train_loss:1.0562519740175318, valid_loss:1.2886595129966736, test_acc:0.815267175572519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.77it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 35.10it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.62it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 13.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18, train_loss:1.018346987388752, valid_loss:1.2306369543075562, test_acc:0.8183206106870229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.06it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.45it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.53it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19, train_loss:0.9539690017700195, valid_loss:1.2030708193778992, test_acc:0.8244274809160306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.69it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 35.44it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 35.06it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 13.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20, train_loss:0.8786712333008095, valid_loss:1.160897672176361, test_acc:0.8229007633587786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.03it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.55it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.33it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21, train_loss:0.8501026122658341, valid_loss:1.1630886793136597, test_acc:0.8183206106870229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.05it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.61it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.51it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:02, 12.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22, train_loss:0.808682797131715, valid_loss:1.1121562123298645, test_acc:0.8305343511450382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.74it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 33.59it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 33.41it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23, train_loss:0.7770996424886916, valid_loss:1.1341722011566162, test_acc:0.8198473282442749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.62it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 33.90it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.17it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24, train_loss:0.7155413439980259, valid_loss:1.1064651608467102, test_acc:0.8366412213740458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.04it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.28it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.39it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25, train_loss:0.6986485057406955, valid_loss:1.154703974723816, test_acc:0.8366412213740458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.06it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.68it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.52it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26, train_loss:0.6514985197120242, valid_loss:1.1779365539550781, test_acc:0.833587786259542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.06it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.42it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.51it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27, train_loss:0.6149280071258545, valid_loss:1.103727638721466, test_acc:0.8458015267175573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.02it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.31it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.43it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28, train_loss:0.5374401289003866, valid_loss:1.1724441349506378, test_acc:0.8351145038167939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.77it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.10it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.16it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29, train_loss:0.5435559661300094, valid_loss:1.15543931722641, test_acc:0.833587786259542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.02it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.05it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.37it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30, train_loss:0.5155844390392303, valid_loss:1.1258782148361206, test_acc:0.8351145038167939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.04it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.28it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.04it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31, train_loss:0.516019156685582, valid_loss:1.1507461667060852, test_acc:0.8396946564885496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.03it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.27it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.18it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32, train_loss:0.44699162355175726, valid_loss:1.220868706703186, test_acc:0.8305343511450382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.94it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.86it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.28it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33, train_loss:0.4541569837817439, valid_loss:1.1318506002426147, test_acc:0.8366412213740458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 13.02it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.68it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.27it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34, train_loss:0.4021299680074056, valid_loss:1.104996144771576, test_acc:0.8351145038167939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.64it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.71it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.36it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35, train_loss:0.375795422328843, valid_loss:1.1957168579101562, test_acc:0.833587786259542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.62it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 35.16it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.09it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36, train_loss:0.3660257994024842, valid_loss:1.1234041452407837, test_acc:0.8320610687022901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.94it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.51it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.38it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 12.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37, train_loss:0.338266560876811, valid_loss:1.1783284842967987, test_acc:0.8412213740458016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.98it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.23it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 34.48it/s]\n",
      "Iteration:   7%|▋         | 2/27 [00:00<00:01, 13.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38, train_loss:0.32681266246018587, valid_loss:1.1483153104782104, test_acc:0.8381679389312977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 27/27 [00:02<00:00, 12.91it/s]\n",
      "Iteration: 100%|██████████| 2/2 [00:00<00:00, 34.54it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 33.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39, train_loss:0.31838869441438605, valid_loss:1.1747119724750519, test_acc:0.8381679389312977\n",
      "Total Result:\n",
      "best_accuracy:  0.8458015267175573\n",
      "best loss:  1.103727638721466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    train_data_loader,\n",
    "    dev_data_loader,\n",
    "    test_data_loader,\n",
    "    num_train_optimization_steps,\n",
    "    train_tokenizer,\n",
    "    dev_tokenizer,\n",
    "    test_tokenizer\n",
    ") = set_up_data_loader()\n",
    "\n",
    "model, optimizer, scheduler = prep_for_training(\n",
    "    num_train_optimization_steps)\n",
    "\n",
    "model = train(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    dev_data_loader,\n",
    "    test_data_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    test_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Intensity Reflection of Fustion Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-93ae8bdca532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mpred_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mtest_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "with open(f\"../datasets/{args.dataset}.pkl\", \"rb\") as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "train_data = data[\"train\"]\n",
    "dev_data = data[\"dev\"]\n",
    "test_data = data[\"test\"]\n",
    "\n",
    "train_dataset, train_tokenizer = get_appropriate_dataset(train_data)\n",
    "dev_dataset, dev_tokenizer = get_appropriate_dataset(dev_data)\n",
    "test_dataset, test_tokenizer = get_appropriate_dataset(test_data)\n",
    "\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_embeddings = torch.zeros((0, 100), dtype=torch.float32)\n",
    "preds = []\n",
    "labels = []\n",
    "classes = []\n",
    "pred_classes = []\n",
    "\n",
    "# Gold 7-Class\n",
    "for idx in range(len(test_data)):\n",
    "    (word, visual, acoustic), label, segment = test_data[idx]\n",
    "    if label < -15/7:\n",
    "        classes.append(-3)\n",
    "    elif label < -9/7:\n",
    "        classes.append(-2)\n",
    "    elif label < -3/7:\n",
    "        classes.append(-1)\n",
    "    elif label < 3/7:\n",
    "        classes.append(0)\n",
    "    elif label < 9/7:\n",
    "        classes.append(1)\n",
    "    elif label < 15/7:\n",
    "        classes.append(2)\n",
    "    else:\n",
    "        classes.append(3)\n",
    "classes = np.array(classes)\n",
    "\n",
    "# MAG-BERT Model output\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(test_data_loader)):\n",
    "        batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "        input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "        visual = torch.squeeze(visual, 1)\n",
    "        acoustic = torch.squeeze(acoustic, 1)\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            visual,\n",
    "            acoustic,\n",
    "            token_type_ids=segment_ids,\n",
    "            attention_mask=input_mask,\n",
    "            labels=None\n",
    "        )\n",
    "\n",
    "        logits = outputs[0]\n",
    "        embeddings = outputs[1:]\n",
    "\n",
    "        test_embeddings = torch.cat((test_embeddings, embeddings.detach().cpu()), 0)\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.detach().cpu().numpy()\n",
    "\n",
    "        preds.extend(np.squeeze(logits).tolist())\n",
    "        labels.extend(np.squeeze(label_ids).tolist())\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # 7-class prediction\n",
    "        for logit in logits:\n",
    "            if logit < -15/7:\n",
    "                pred_classes.append(-3)\n",
    "            elif logit < -9/7:\n",
    "                pred_classes.append(-2)\n",
    "            elif logit < -3/7:\n",
    "                pred_classes.append(-1)\n",
    "            elif logit < 3/7:\n",
    "                pred_classes.append(0)\n",
    "            elif logit < 9/7:\n",
    "                pred_classes.append(1)\n",
    "            elif logit < 15/7:\n",
    "                pred_classes.append(2)\n",
    "            else:\n",
    "                pred_classes.append(3)\n",
    "        pred_classes = np.array(pred_classes)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a two dimensional t-SNE projection of the embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(2, verbose=1)\n",
    "tsne_proj = tsne.fit_transform(test_embeddings)\n",
    "cmap = cm.get_cmap('tab20')\n",
    "fig, ax = plt.subplot(figsize=(8,8))\n",
    "# num_categories = 7\n",
    "for lab in range(-3, 3):\n",
    "    indices = pred_classes==lab\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99b221e9db0318903f19c34b1ba0eab49364574753058918f60cea91a58935ca"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('pytorch1.7.1_p37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
