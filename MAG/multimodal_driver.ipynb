{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from typing import *\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, L1Loss, MSELoss\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from transformers import BertTokenizer, XLNetTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "from transformers.optimization import AdamW\n",
    "from bert import MAG_BertForSequenceClassification\n",
    "from xlnet import MAG_XLNetForSequenceClassification\n",
    "\n",
    "from argparse_utils import str2bool, seed\n",
    "from global_configs import ACOUSTIC_DIM, VISUAL_DIM, DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"dataset\": \"mosi\",\n",
    "    \"max_seq_length\": 50,\n",
    "    \"train_batch_size\": 48,\n",
    "    \"dev_batch_size\" : 128,\n",
    "    \"test_batch_size\": 128,\n",
    "    \"n_epochs\": 40,\n",
    "    \"beta_shift\": 1.0,\n",
    "    \"dropout_prob\": 0.5,\n",
    "    \"model\": \"bert-base-uncased\",\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"gradient_accumulation_step\": 1,\n",
    "    \"warmup_proportion\": 0.1,\n",
    "    \"seed\": seed(\"random\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, visual, acoustic, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.visual = visual\n",
    "        self.acoustic = acoustic\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalConfig(object):\n",
    "    def __init__(self, beta_shift, dropout_prob):\n",
    "        self.beta_shift = beta_shift\n",
    "        self.dropout_prob = dropout_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(examples, max_seq_length, tokenizer):\n",
    "    features = []\n",
    "\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "\n",
    "        (words, visual, acoustic), label_id, segment = example\n",
    "\n",
    "        tokens, inversions = [], []\n",
    "        for idx, word in enumerate(words):\n",
    "            tokenized = tokenizer.tokenize(word)\n",
    "            tokens.extend(tokenized)\n",
    "            inversions.extend([idx] * len(tokenized))\n",
    "\n",
    "        # Check inversion\n",
    "        assert len(tokens) == len(inversions)\n",
    "\n",
    "        aligned_visual = []\n",
    "        aligned_audio = []\n",
    "\n",
    "        for inv_idx in inversions:\n",
    "            aligned_visual.append(visual[inv_idx, :])\n",
    "            aligned_audio.append(acoustic[inv_idx, :])\n",
    "\n",
    "        visual = np.array(aligned_visual)\n",
    "        acoustic = np.array(aligned_audio)\n",
    "\n",
    "        # Truncate input if necessary\n",
    "        if len(tokens) > max_seq_length - 2:\n",
    "            tokens = tokens[: max_seq_length - 2]\n",
    "            acoustic = acoustic[: max_seq_length - 2]\n",
    "            visual = visual[: max_seq_length - 2]\n",
    "\n",
    "        if args.model == \"bert-base-uncased\":\n",
    "            prepare_input = prepare_bert_input\n",
    "        elif args.model == \"xlnet-base-cased\":\n",
    "            prepare_input = prepare_xlnet_input\n",
    "\n",
    "        input_ids, visual, acoustic, input_mask, segment_ids = prepare_input(\n",
    "            tokens, visual, acoustic, tokenizer\n",
    "        )\n",
    "\n",
    "        # Check input length\n",
    "        assert len(input_ids) == args.max_seq_length\n",
    "        assert len(input_mask) == args.max_seq_length\n",
    "        assert len(segment_ids) == args.max_seq_length\n",
    "        assert acoustic.shape[0] == args.max_seq_length\n",
    "        assert visual.shape[0] == args.max_seq_length\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                visual=visual,\n",
    "                acoustic=acoustic,\n",
    "                label_id=label_id,\n",
    "            )\n",
    "        )\n",
    "    return features\n",
    "\n",
    "\n",
    "def prepare_bert_input(tokens, visual, acoustic, tokenizer):\n",
    "    CLS = tokenizer.cls_token\n",
    "    SEP = tokenizer.sep_token\n",
    "    tokens = [CLS] + tokens + [SEP]\n",
    "\n",
    "    # Pad zero vectors for acoustic / visual vectors to account for [CLS] / [SEP] tokens\n",
    "    acoustic_zero = np.zeros((1, ACOUSTIC_DIM))\n",
    "    acoustic = np.concatenate((acoustic_zero, acoustic, acoustic_zero))\n",
    "    visual_zero = np.zeros((1, VISUAL_DIM))\n",
    "    visual = np.concatenate((visual_zero, visual, visual_zero))\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    segment_ids = [0] * len(input_ids)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    pad_length = args.max_seq_length - len(input_ids)\n",
    "\n",
    "    acoustic_padding = np.zeros((pad_length, ACOUSTIC_DIM))\n",
    "    acoustic = np.concatenate((acoustic, acoustic_padding))\n",
    "\n",
    "    visual_padding = np.zeros((pad_length, VISUAL_DIM))\n",
    "    visual = np.concatenate((visual, visual_padding))\n",
    "\n",
    "    padding = [0] * pad_length\n",
    "\n",
    "    # Pad inputs\n",
    "    input_ids += padding\n",
    "    input_mask += padding\n",
    "    segment_ids += padding\n",
    "\n",
    "    return input_ids, visual, acoustic, input_mask, segment_ids\n",
    "\n",
    "\n",
    "def prepare_xlnet_input(tokens, visual, acoustic, tokenizer):\n",
    "    CLS = tokenizer.cls_token\n",
    "    SEP = tokenizer.sep_token\n",
    "    PAD_ID = tokenizer.pad_token_id\n",
    "\n",
    "    # PAD special tokens\n",
    "    tokens = tokens + [SEP] + [CLS]\n",
    "    audio_zero = np.zeros((1, ACOUSTIC_DIM))\n",
    "    acoustic = np.concatenate((acoustic, audio_zero, audio_zero))\n",
    "    visual_zero = np.zeros((1, VISUAL_DIM))\n",
    "    visual = np.concatenate((visual, visual_zero, visual_zero))\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    segment_ids = [0] * (len(tokens) - 1) + [2]\n",
    "\n",
    "    pad_length = (args.max_seq_length - len(segment_ids))\n",
    "\n",
    "    # then zero pad the visual and acoustic\n",
    "    audio_padding = np.zeros((pad_length, ACOUSTIC_DIM))\n",
    "    acoustic = np.concatenate((audio_padding, acoustic))\n",
    "\n",
    "    video_padding = np.zeros((pad_length, VISUAL_DIM))\n",
    "    visual = np.concatenate((video_padding, visual))\n",
    "\n",
    "    input_ids = [PAD_ID] * pad_length + input_ids\n",
    "    input_mask = [0] * pad_length + input_mask\n",
    "    segment_ids = [3] * pad_length + segment_ids\n",
    "\n",
    "    return input_ids, visual, acoustic, input_mask, segment_ids\n",
    "\n",
    "\n",
    "def get_tokenizer(model):\n",
    "    if model == \"bert-base-uncased\":\n",
    "        return BertTokenizer.from_pretrained(model)\n",
    "    elif model == \"xlnet-base-cased\":\n",
    "        return XLNetTokenizer.from_pretrained(model)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Expected 'bert-base-uncased' or 'xlnet-base-cased, but received {}\".format(\n",
    "                model\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def get_appropriate_dataset(data):\n",
    "\n",
    "    tokenizer = get_tokenizer(args.model)\n",
    "\n",
    "    features = convert_to_features(data, args.max_seq_length, tokenizer)\n",
    "    all_input_ids = torch.tensor(\n",
    "        [f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor(\n",
    "        [f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor(\n",
    "        [f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_visual = torch.tensor([f.visual for f in features], dtype=torch.float)\n",
    "    all_acoustic = torch.tensor(\n",
    "        [f.acoustic for f in features], dtype=torch.float)\n",
    "    all_label_ids = torch.tensor(\n",
    "        [f.label_id for f in features], dtype=torch.float)\n",
    "\n",
    "    dataset = TensorDataset(\n",
    "        all_input_ids,\n",
    "        all_visual,\n",
    "        all_acoustic,\n",
    "        all_input_mask,\n",
    "        all_segment_ids,\n",
    "        all_label_ids,\n",
    "    )\n",
    "    return dataset, tokenizer\n",
    "\n",
    "\n",
    "def set_up_data_loader():\n",
    "    with open(f\"../datasets/{args.dataset}.pkl\", \"rb\") as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    train_data = data[\"train\"]\n",
    "    dev_data = data[\"dev\"]\n",
    "    test_data = data[\"test\"]\n",
    "\n",
    "    train_dataset, train_tokenizer = get_appropriate_dataset(train_data)\n",
    "    dev_dataset, dev_tokenizer = get_appropriate_dataset(dev_data)\n",
    "    test_dataset, test_tokenizer = get_appropriate_dataset(test_data)\n",
    "\n",
    "    num_train_optimization_steps = (\n",
    "        int(\n",
    "            len(train_dataset) / args.train_batch_size /\n",
    "            args.gradient_accumulation_step\n",
    "        )\n",
    "        * args.n_epochs\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=args.train_batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    dev_dataloader = DataLoader(\n",
    "        dev_dataset, batch_size=args.dev_batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=args.test_batch_size, shuffle=True,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        train_dataloader,\n",
    "        dev_dataloader,\n",
    "        test_dataloader,\n",
    "        num_train_optimization_steps,\n",
    "        train_tokenizer,\n",
    "        dev_tokenizer,\n",
    "        test_tokenizer\n",
    "    )\n",
    "\n",
    "\n",
    "def set_random_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function to seed experiment for reproducibility.\n",
    "    If -1 is provided as seed, experiment uses random seed from 0~9999\n",
    "\n",
    "    Args:\n",
    "        seed (int): integer to be used as seed, use -1 to randomly seed experiment\n",
    "    \"\"\"\n",
    "    print(\"Seed: {}\".format(seed))\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def prep_for_training(num_train_optimization_steps: int):\n",
    "    multimodal_config = MultimodalConfig(\n",
    "        beta_shift=args.beta_shift, dropout_prob=args.dropout_prob\n",
    "    )\n",
    "    bert_config = BertConfig(\n",
    "        hidden_dropout_prob=args.dropout_prob\n",
    "    )\n",
    "\n",
    "    if args.model == \"bert-base-uncased\":\n",
    "        model = MAG_BertForSequenceClassification.from_pretrained(\n",
    "            args.model, multimodal_config=multimodal_config, num_labels=1,\n",
    "        )\n",
    "        # model = BertForSequenceClassification.from_pretrained(\n",
    "        #     args.model,\n",
    "        #     num_labels = 1\n",
    "        # )\n",
    "    elif args.model == \"xlnet-base-cased\":\n",
    "        model = MAG_XLNetForSequenceClassification.from_pretrained(\n",
    "            args.model, multimodal_config=multimodal_config, num_labels=1\n",
    "        )\n",
    "\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Prepare optimizer\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_train_optimization_steps,\n",
    "        num_training_steps=args.warmup_proportion * num_train_optimization_steps,\n",
    "    )\n",
    "    return model, optimizer, scheduler\n",
    "\n",
    "\n",
    "def train_epoch(model: nn.Module, train_dataloader: DataLoader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "        batch = tuple(t.to(DEVICE) for t in batch)\n",
    "        input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "        visual = torch.squeeze(visual, 1)\n",
    "        acoustic = torch.squeeze(acoustic, 1)\n",
    "        model.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            visual,\n",
    "            acoustic,\n",
    "            token_type_ids=segment_ids,\n",
    "            attention_mask=input_mask,\n",
    "            labels=None\n",
    "        )\n",
    "\n",
    "        logits = outputs[0]\n",
    "        loss_fct = MSELoss()\n",
    "        loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "        if args.gradient_accumulation_step > 1:\n",
    "            loss = loss / args.gradient_accumulation_step\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        if (step + 1) % args.gradient_accumulation_step == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    return tr_loss / nb_tr_steps\n",
    "\n",
    "\n",
    "def eval_epoch(model: nn.Module, dev_dataloader: DataLoader, optimizer):\n",
    "    model.eval()\n",
    "    dev_loss = 0\n",
    "    nb_dev_examples, nb_dev_steps = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(tqdm(dev_dataloader, desc=\"Iteration\")):\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "            input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "            visual = torch.squeeze(visual, 1)\n",
    "            acoustic = torch.squeeze(acoustic, 1)\n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                visual,\n",
    "                acoustic,\n",
    "                token_type_ids=segment_ids,\n",
    "                attention_mask=input_mask,\n",
    "                labels=None\n",
    "            )\n",
    "            logits = outputs[0]\n",
    "\n",
    "            loss_fct = MSELoss()\n",
    "            loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "            if args.gradient_accumulation_step > 1:\n",
    "                loss = loss / args.gradient_accumulation_step\n",
    "\n",
    "            dev_loss += loss.item()\n",
    "            nb_dev_steps += 1\n",
    "\n",
    "    return dev_loss / nb_dev_steps\n",
    "\n",
    "\n",
    "def test_epoch(model: nn.Module, test_dataloader: DataLoader, tokenizer):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_dataloader)):\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "            input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "            visual = torch.squeeze(visual, 1)\n",
    "            acoustic = torch.squeeze(acoustic, 1)\n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                visual,\n",
    "                acoustic,\n",
    "                token_type_ids=segment_ids,\n",
    "                attention_mask=input_mask,\n",
    "                labels=None\n",
    "            )\n",
    "\n",
    "            logits = outputs[0]\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.detach().cpu().numpy()\n",
    "\n",
    "            logits = np.squeeze(logits).tolist()\n",
    "            label_ids = np.squeeze(label_ids).tolist()\n",
    "\n",
    "            preds.extend(logits)\n",
    "            labels.extend(label_ids)\n",
    "\n",
    "            # print(i, \" th batch\")\n",
    "            # for i, s in enumerate(input_ids):\n",
    "            #     tokens = tokenizer.convert_ids_to_tokens(s, skip_special_tokens = True)\n",
    "            #     print(tokens, logits[i], label_ids[i])\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # for s in input_ids:\n",
    "        #     tokens = tokenizer.convert_ids_to_tokens(s)\n",
    "        #     print(tokens)\n",
    "        # print(preds)\n",
    "        # print(labels)\n",
    "        # print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def test_score_model(model: nn.Module, test_dataloader: DataLoader, tokenizer, use_zero=False):\n",
    "\n",
    "    preds, y_test = test_epoch(model, test_dataloader, tokenizer)\n",
    "    non_zeros = np.array(\n",
    "        [i for i, e in enumerate(y_test) if e != 0 or use_zero])\n",
    "\n",
    "    preds = preds[non_zeros]\n",
    "    y_test = y_test[non_zeros]\n",
    "\n",
    "    mae = np.mean(np.absolute(preds - y_test))\n",
    "    corr = np.corrcoef(preds, y_test)[0][1]\n",
    "\n",
    "    preds = preds >= 0\n",
    "    y_test = y_test >= 0\n",
    "\n",
    "    f_score = f1_score(y_test, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "\n",
    "    return acc, mae, corr, f_score\n",
    "\n",
    "\n",
    "def test_instance(model: nn.Module, test_tokenizer):\n",
    "    model.eval()\n",
    "    segment_list = []\n",
    "    words_list = []\n",
    "    preds = []\n",
    "    preds_2 = []\n",
    "    preds_7 = []\n",
    "    labels = []\n",
    "    labels_2 = []\n",
    "    labels_7 = []\n",
    "\n",
    "    with open(f\"../datasets/{args.dataset}.pkl\", \"rb\") as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    # test_data[idx] = (words, visual, acoustic), label, segment\n",
    "    test_data = data[\"test\"]\n",
    "    test_dataset, test_tokenizer = get_appropriate_dataset(test_data)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=args.test_batch_size, shuffle=False,\n",
    "    )\n",
    "\n",
    "    video = set()\n",
    "    count = 0\n",
    "\n",
    "    for idx in range(len(test_data)):\n",
    "        (words, visual, acoustic), label, segment = test_data[idx]\n",
    "        if args.dataset == 'mosi':\n",
    "            segment_list.append(segment)\n",
    "        else:\n",
    "            video_name = segment[0]\n",
    "            if video_name in video:\n",
    "                count += 1\n",
    "            else:\n",
    "                video.add(video_name)\n",
    "                count = 0\n",
    "            segment_list.append(video_name + '[' + str(count) + ']')\n",
    "\n",
    "        words_list.append(words)\n",
    "        labels.append(label[0][0])\n",
    "\n",
    "        # label_2 appending\n",
    "        if label > 0:\n",
    "            labels_2.append('positive')\n",
    "        else:\n",
    "            labels_2.append('negative')\n",
    "        \n",
    "        # label_7 appending\n",
    "        if label < -15/7:\n",
    "            labels_7.append('very negative')\n",
    "        elif label < -9/7:\n",
    "            labels_7.append('negative')\n",
    "        elif label < -3/7:\n",
    "            labels_7.append('slightly negative')\n",
    "        elif label < 3/7:\n",
    "            labels_7.append('Neutral')\n",
    "        elif label < 9/7:\n",
    "            labels_7.append('slightly positive')\n",
    "        elif label < 15/7:\n",
    "            labels_7.append('positive')\n",
    "        else:\n",
    "            labels_7.append('very positive')\n",
    "            \n",
    "    # prediction\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_dataloader)):\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "            input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "            visual = torch.squeeze(visual, 1)\n",
    "            acoustic = torch.squeeze(acoustic, 1)\n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                visual,\n",
    "                acoustic,\n",
    "                token_type_ids=segment_ids,\n",
    "                attention_mask=input_mask,\n",
    "                labels=None\n",
    "            )\n",
    "            logits = outputs[0]\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.detach().cpu().numpy()\n",
    "\n",
    "            logits = np.squeeze(logits).tolist()\n",
    "            label_ids = np.squeeze(label_ids).tolist()\n",
    "\n",
    "            preds.extend(logits)\n",
    "\n",
    "            for logit in logits:\n",
    "                # preds_2 appending\n",
    "                if logit > 0:\n",
    "                    preds_2.append('positive')\n",
    "                else:\n",
    "                    preds_2.append('negative')\n",
    "\n",
    "                # label_7 appending\n",
    "                if logit < -15/7:\n",
    "                    preds_7.append('very negative')\n",
    "                elif logit < -9/7:\n",
    "                    preds_7.append('negative')\n",
    "                elif logit < -3/7:\n",
    "                    preds_7.append('slightly negative')\n",
    "                elif logit < 3/7:\n",
    "                    preds_7.append('Neutral')\n",
    "                elif logit < 9/7:\n",
    "                    preds_7.append('slightly positive')\n",
    "                elif logit < 15/7:\n",
    "                    preds_7.append('positive')\n",
    "                else:\n",
    "                    preds_7.append('very positive')\n",
    "\n",
    "            \n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(segment_list)):\n",
    "        print(i, \"th data\")\n",
    "        print(segment_list[i])\n",
    "        print(words_list[i])\n",
    "        print(labels[i])\n",
    "        print(labels_2[i])\n",
    "        print(labels_7[i])\n",
    "        print(preds[i])\n",
    "        print(preds_2[i])\n",
    "        print(preds_7[i])\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    validation_dataloader,\n",
    "    test_data_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    tokenizer\n",
    "):\n",
    "    valid_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch_i in range(int(args.n_epochs)):\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, scheduler)\n",
    "        valid_loss = eval_epoch(model, validation_dataloader, optimizer)\n",
    "        test_acc, test_mae, test_corr, test_f_score = test_score_model(\n",
    "            model, test_data_loader, tokenizer\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"epoch:{}, train_loss:{}, valid_loss:{}, test_acc:{}\".format(\n",
    "                epoch_i, train_loss, valid_loss, test_acc\n",
    "            )\n",
    "        )\n",
    "\n",
    "        valid_losses.append(valid_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "    \n",
    "    print(\"Total Result:\")\n",
    "    print(\"best_accuracy: \", sorted(test_accuracies)[-1])\n",
    "    print(\"best loss: \", sorted(valid_losses)[0])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Seed: 186\n"
=======
      "Seed: 9909\n"
>>>>>>> a5d07afc40a35a4108e21bd6bf36a03b0a32900f
     ]
    }
   ],
   "source": [
    "set_random_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MAG with beta_shift:1.0 hidden_prob:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing MAG_BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MAG_BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MAG_BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MAG_BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.MAG.W_hv.weight', 'bert.MAG.W_ha.bias', 'bert.MAG.W_hv.bias', 'classifier.bias', 'bert.MAG.W_ha.weight', 'bert.MAG.W_a.bias', 'bert.MAG.LayerNorm.weight', 'bert.MAG.W_v.weight', 'bert.MAG.W_a.weight', 'bert.MAG.LayerNorm.bias', 'bert.MAG.W_v.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fd0ca6772d8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m model, optimizer, scheduler = prep_for_training(\n\u001b[0;32m---> 12\u001b[0;31m     num_train_optimization_steps)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m model = train(\n",
      "\u001b[0;32m<ipython-input-5-43c4244d1f55>\u001b[0m in \u001b[0;36mprep_for_training\u001b[0;34m(num_train_optimization_steps)\u001b[0m\n\u001b[1;32m    252\u001b[0m         )\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;31m# Prepare optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch1.7.1_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch1.7.1_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch1.7.1_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch1.7.1_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch1.7.1_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch1.7.1_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch1.7.1_p37/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "(\n",
    "    train_data_loader,\n",
    "    dev_data_loader,\n",
    "    test_data_loader,\n",
    "    num_train_optimization_steps,\n",
    "    train_tokenizer,\n",
    "    dev_tokenizer,\n",
    "    test_tokenizer\n",
    ") = set_up_data_loader()\n",
    "\n",
    "model, optimizer, scheduler = prep_for_training(\n",
    "    num_train_optimization_steps)\n",
    "\n",
    "model = train(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    dev_data_loader,\n",
    "    test_data_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    test_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./saved_models_MAG_mosi.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MAG with beta_shift:1.0 hidden_prob:0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing MAG_BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing MAG_BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MAG_BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MAG_BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.MAG.LayerNorm.bias', 'bert.MAG.W_hv.weight', 'bert.MAG.W_v.bias', 'classifier.weight', 'bert.MAG.LayerNorm.weight', 'bert.MAG.W_a.bias', 'bert.MAG.W_a.weight', 'bert.MAG.W_v.weight', 'classifier.bias', 'bert.MAG.W_hv.bias', 'bert.MAG.W_ha.bias', 'bert.MAG.W_ha.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MAG_BertForSequenceClassification(\n",
       "  (bert): MAG_BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (MAG): MAG(\n",
       "      (W_hv): Linear(in_features=815, out_features=768, bias=True)\n",
       "      (W_ha): Linear(in_features=842, out_features=768, bias=True)\n",
       "      (W_v): Linear(in_features=47, out_features=768, bias=True)\n",
       "      (W_a): Linear(in_features=74, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_config = MultimodalConfig(beta_shift=args.beta_shift, dropout_prob=args.dropout_prob)\n",
    "bert_config = BertConfig(hidden_dropout_prob=args.dropout_prob)\n",
    "model = MAG_BertForSequenceClassification.from_pretrained(args.model, multimodal_config=multimodal_config, num_labels=1)\n",
    "model.load_state_dict(torch.load(\"./saved_models_MAG_mosi.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Intensity Reflection of Fustion Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-93ae8bdca532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mpred_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mtest_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "with open(f\"../datasets/{args.dataset}.pkl\", \"rb\") as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "train_data = data[\"train\"]\n",
    "dev_data = data[\"dev\"]\n",
    "test_data = data[\"test\"]\n",
    "\n",
    "train_dataset, train_tokenizer = get_appropriate_dataset(train_data)\n",
    "dev_dataset, dev_tokenizer = get_appropriate_dataset(dev_data)\n",
    "test_dataset, test_tokenizer = get_appropriate_dataset(test_data)\n",
    "\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_embeddings = torch.zeros((0, 100), dtype=torch.float32)\n",
    "preds = []\n",
    "labels = []\n",
    "classes = []\n",
    "pred_classes = []\n",
    "\n",
    "# Gold 7-Class\n",
    "for idx in range(len(test_data)):\n",
    "    (word, visual, acoustic), label, segment = test_data[idx]\n",
    "    if label < -15/7:\n",
    "        classes.append(-3)\n",
    "    elif label < -9/7:\n",
    "        classes.append(-2)\n",
    "    elif label < -3/7:\n",
    "        classes.append(-1)\n",
    "    elif label < 3/7:\n",
    "        classes.append(0)\n",
    "    elif label < 9/7:\n",
    "        classes.append(1)\n",
    "    elif label < 15/7:\n",
    "        classes.append(2)\n",
    "    else:\n",
    "        classes.append(3)\n",
    "classes = np.array(classes)\n",
    "\n",
    "# MAG-BERT Model output\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(test_data_loader)):\n",
    "        batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "        input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "        visual = torch.squeeze(visual, 1)\n",
    "        acoustic = torch.squeeze(acoustic, 1)\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            visual,\n",
    "            acoustic,\n",
    "            token_type_ids=segment_ids,\n",
    "            attention_mask=input_mask,\n",
    "            labels=None\n",
    "        )\n",
    "\n",
    "        logits = outputs[0]\n",
    "        embeddings = outputs[1:]\n",
    "\n",
    "        test_embeddings = torch.cat((test_embeddings, embeddings.detach().cpu()), 0)\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.detach().cpu().numpy()\n",
    "\n",
    "        preds.extend(np.squeeze(logits).tolist())\n",
    "        labels.extend(np.squeeze(label_ids).tolist())\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # 7-class prediction\n",
    "        for logit in logits:\n",
    "            if logit < -15/7:\n",
    "                pred_classes.append(-3)\n",
    "            elif logit < -9/7:\n",
    "                pred_classes.append(-2)\n",
    "            elif logit < -3/7:\n",
    "                pred_classes.append(-1)\n",
    "            elif logit < 3/7:\n",
    "                pred_classes.append(0)\n",
    "            elif logit < 9/7:\n",
    "                pred_classes.append(1)\n",
    "            elif logit < 15/7:\n",
    "                pred_classes.append(2)\n",
    "            else:\n",
    "                pred_classes.append(3)\n",
    "        pred_classes = np.array(pred_classes)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a two dimensional t-SNE projection of the embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(2, verbose=1)\n",
    "tsne_proj = tsne.fit_transform(test_embeddings)\n",
    "cmap = cm.get_cmap('tab20')\n",
    "fig, ax = plt.subplot(figsize=(8,8))\n",
    "# num_categories = 7\n",
    "for lab in range(-3, 3):\n",
    "    indices = pred_classes==lab\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Instance Prediction Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51235/664660039.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mtest_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msegment_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwords_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "def test_instance(model: nn.Module):\n",
    "    model.eval()\n",
    "    segment_list = []\n",
    "    words_list = []\n",
    "    preds = []\n",
    "    preds_2 = []\n",
    "    preds_7 = []\n",
    "    labels = []\n",
    "    labels_2 = []\n",
    "    labels_7 = []\n",
    "\n",
    "    with open(f\"../datasets/{args.dataset}.pkl\", \"rb\") as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    # test_data[idx] = (words, visual, acoustic), label, segment\n",
    "    test_data = data[\"test\"]\n",
    "    test_dataset, test_tokenizer = get_appropriate_dataset(test_data)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=args.test_batch_size, shuffle=False,\n",
    "    )\n",
    "\n",
    "    video = set()\n",
    "    count = 0\n",
    "\n",
    "    for idx in range(len(test_data)):\n",
    "        (words, visual, acoustic), label, segment = test_data[idx]\n",
    "        if args.dataset == 'mosi':\n",
    "            segment_list.append(segment)\n",
    "        else:\n",
    "            video_name = segment[0]\n",
    "            if video_name in video:\n",
    "                count += 1\n",
    "            else:\n",
    "                video.add(video_name)\n",
    "                count = 0\n",
    "            segment_list.append(video_name + '[' + str(count) + ']')\n",
    "\n",
    "        words_list.append(words)\n",
    "        labels.append(label[0][0])\n",
    "\n",
    "        # label_2 appending\n",
    "        if label > 0:\n",
    "            labels_2.append('positive')\n",
    "        else:\n",
    "            labels_2.append('negative')\n",
    "        \n",
    "        # label_7 appending\n",
    "        if label < -15/7:\n",
    "            labels_7.append('very negative')\n",
    "        elif label < -9/7:\n",
    "            labels_7.append('negative')\n",
    "        elif label < -3/7:\n",
    "            labels_7.append('slightly negative')\n",
    "        elif label < 3/7:\n",
    "            labels_7.append('Neutral')\n",
    "        elif label < 9/7:\n",
    "            labels_7.append('slightly positive')\n",
    "        elif label < 15/7:\n",
    "            labels_7.append('positive')\n",
    "        else:\n",
    "            labels_7.append('very positive')\n",
    "            \n",
    "    # prediction\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_dataloader)):\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "            input_ids, visual, acoustic, input_mask, segment_ids, label_ids = batch\n",
    "            visual = torch.squeeze(visual, 1)\n",
    "            acoustic = torch.squeeze(acoustic, 1)\n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                visual,\n",
    "                acoustic,\n",
    "                token_type_ids=segment_ids,\n",
    "                attention_mask=input_mask,\n",
    "                labels=None\n",
    "            )\n",
    "            logits = outputs[0]\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.detach().cpu().numpy()\n",
    "\n",
    "            logits = np.squeeze(logits).tolist()\n",
    "            label_ids = np.squeeze(label_ids).tolist()\n",
    "\n",
    "            preds.extend(logits)\n",
    "\n",
    "            for logit in logits:\n",
    "                # preds_2 appending\n",
    "                if logit > 0:\n",
    "                    preds_2.append('positive')\n",
    "                else:\n",
    "                    preds_2.append('negative')\n",
    "\n",
    "                # label_7 appending\n",
    "                if logit < -15/7:\n",
    "                    preds_7.append('very negative')\n",
    "                elif logit < -9/7:\n",
    "                    preds_7.append('negative')\n",
    "                elif logit < -3/7:\n",
    "                    preds_7.append('slightly negative')\n",
    "                elif logit < 3/7:\n",
    "                    preds_7.append('Neutral')\n",
    "                elif logit < 9/7:\n",
    "                    preds_7.append('slightly positive')\n",
    "                elif logit < 15/7:\n",
    "                    preds_7.append('positive')\n",
    "                else:\n",
    "                    preds_7.append('very positive')\n",
    "\n",
    "                # if logit < -15/7:\n",
    "                #     preds_7.append('-3')\n",
    "                # elif logit < -9/7:\n",
    "                #     preds_7.append('-2')\n",
    "                # elif logit < -3/7:\n",
    "                #     preds_7.append('-1')\n",
    "                # elif logit < 3/7:\n",
    "                #     preds_7.append('0')\n",
    "                # elif logit < 9/7:\n",
    "                #     preds_7.append('1')\n",
    "                # elif logit < 15/7:\n",
    "                #     preds_7.append('2')\n",
    "                # else:\n",
    "                #     preds_7.append('3')\n",
    "\n",
    "            \n",
    "\n",
    "    count = 0\n",
    "    # for i in range(len(segment_list)):\n",
    "    #     print(i, \"th data\")\n",
    "    #     print(segment_list[i])\n",
    "    #     print(words_list[i])\n",
    "    #     print(labels[i])\n",
    "    #     print(labels_2[i])\n",
    "    #     print(labels_7[i])\n",
    "    #     print(preds[i])\n",
    "    #     print(preds_2[i])\n",
    "    #     print(preds_7[i])\n",
    "\n",
    "    return segment_list, words_list, labels, labels_2, labels_7, preds, preds_2, preds_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/soyeon/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:151: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "100%|██████████| 6/6 [00:00<00:00,  6.91it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.to(DEVICE)\n",
    "segment_list, words_list, labels, labels_2, labels_7, preds, preds_2, preds_7 = test_instance(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a202f80022aa4624b59b37b98ec14796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='idx', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def get_predict_result(idx = range(len(segment_list))):\n",
    "    print(\"SEGMENT:\", segment_list[idx])\n",
    "    print(\"WORDS:\", words_list[idx])\n",
    "    print(\"GOLD_VALUE:\", labels[idx])\n",
    "    print(\"GOLD_BINARY:\", labels_2[idx])\n",
    "    print(\"GOLD_7_CLASS:\", labels_7[idx])\n",
    "    print(\"PREDICTED_VALUE:\", preds[idx])\n",
    "    print(\"PREDICTED_BINARY:\", preds_2[idx])\n",
    "    print(\"PREDICTED _7_CLASS:\", preds_7[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "# d = {'segmentID': segment_list, 'words': words_list, 'labels': labels, 'labels_2': labels_2, 'labels_7': labels_7, 'preds': preds, 'preds_2': preds_2, 'preds_7': preds_7}\n",
    "# df = pd.DataFrame(data=d)\n",
    "# order = ['very negative', 'negative', 'slightly negative', 'Neutral', 'slightly positive', 'positive', 'very positive']\n",
    "\n",
    "# sns.set_style('ticks')\n",
    "# p = sns.displot(df, x=\"preds_7\", hue_order=order, kind=\"hist\")\n",
    "# p.set_xticklabels(rotation=30)\n",
    "# p.fig.set_dpi(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "labels_7=%{x}<br>count=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "textposition": "auto",
         "type": "bar",
         "x": [
          "very negative",
          "very negative",
          "slightly negative",
          "positive",
          "very negative",
          "very negative",
          "very negative",
          "very negative",
          "Neutral",
          "slightly positive",
          "slightly positive",
          "very negative",
          "very negative",
          "slightly positive",
          "very negative",
          "negative",
          "negative",
          "very negative",
          "negative",
          "negative",
          "slightly negative",
          "very negative",
          "slightly negative",
          "negative",
          "Neutral",
          "very negative",
          "negative",
          "slightly negative",
          "slightly positive",
          "slightly positive",
          "negative",
          "very negative",
          "negative",
          "very positive",
          "very positive",
          "slightly positive",
          "slightly positive",
          "very positive",
          "very positive",
          "very positive",
          "slightly negative",
          "Neutral",
          "positive",
          "positive",
          "slightly negative",
          "negative",
          "very negative",
          "very negative",
          "very negative",
          "very negative",
          "very positive",
          "slightly negative",
          "positive",
          "positive",
          "slightly positive",
          "slightly positive",
          "positive",
          "very negative",
          "slightly negative",
          "slightly negative",
          "very positive",
          "positive",
          "slightly negative",
          "very negative",
          "very positive",
          "very positive",
          "very positive",
          "Neutral",
          "very positive",
          "positive",
          "slightly negative",
          "positive",
          "Neutral",
          "slightly positive",
          "slightly positive",
          "positive",
          "slightly negative",
          "positive",
          "slightly negative",
          "slightly negative",
          "positive",
          "slightly positive",
          "positive",
          "positive",
          "positive",
          "slightly negative",
          "slightly negative",
          "Neutral",
          "slightly negative",
          "slightly negative",
          "Neutral",
          "slightly positive",
          "slightly negative",
          "slightly negative",
          "Neutral",
          "Neutral",
          "Neutral",
          "slightly negative",
          "slightly positive",
          "slightly positive",
          "very negative",
          "negative",
          "positive",
          "Neutral",
          "positive",
          "slightly negative",
          "very positive",
          "positive",
          "Neutral",
          "very positive",
          "positive",
          "Neutral",
          "very positive",
          "positive",
          "very positive",
          "positive",
          "very positive",
          "positive",
          "positive",
          "slightly positive",
          "Neutral",
          "very negative",
          "negative",
          "Neutral",
          "negative",
          "negative",
          "negative",
          "very negative",
          "very negative",
          "very negative",
          "slightly negative",
          "slightly negative",
          "positive",
          "negative",
          "negative",
          "negative",
          "negative",
          "negative",
          "negative",
          "very negative",
          "slightly positive",
          "very negative",
          "slightly negative",
          "very negative",
          "very negative",
          "negative",
          "very negative",
          "very negative",
          "slightly negative",
          "negative",
          "very negative",
          "negative",
          "slightly negative",
          "very negative",
          "very negative",
          "negative",
          "very negative",
          "very negative",
          "very negative",
          "very negative",
          "slightly negative",
          "very negative",
          "negative",
          "very negative",
          "Neutral",
          "negative",
          "slightly negative",
          "Neutral",
          "slightly positive",
          "slightly positive",
          "negative",
          "slightly positive",
          "slightly negative",
          "slightly positive",
          "slightly negative",
          "negative",
          "slightly negative",
          "negative",
          "slightly positive",
          "Neutral",
          "slightly positive",
          "negative",
          "slightly positive",
          "Neutral",
          "Neutral",
          "slightly negative",
          "slightly negative",
          "very negative",
          "negative",
          "negative",
          "slightly positive",
          "very negative",
          "slightly negative",
          "Neutral",
          "very negative",
          "Neutral",
          "positive",
          "slightly negative",
          "slightly negative",
          "slightly positive",
          "slightly positive",
          "very negative",
          "negative",
          "very negative",
          "slightly negative",
          "negative",
          "very negative",
          "negative",
          "slightly negative",
          "negative",
          "negative",
          "slightly positive",
          "slightly negative",
          "slightly negative",
          "negative",
          "slightly negative",
          "slightly negative",
          "Neutral",
          "positive",
          "positive",
          "positive",
          "very negative",
          "negative",
          "positive",
          "positive",
          "slightly positive",
          "negative",
          "slightly negative",
          "Neutral",
          "positive",
          "slightly positive",
          "slightly positive",
          "slightly positive",
          "positive",
          "Neutral",
          "slightly positive",
          "very negative",
          "slightly positive",
          "Neutral",
          "slightly negative",
          "positive",
          "slightly negative",
          "slightly negative",
          "Neutral",
          "very negative",
          "very negative",
          "very negative",
          "Neutral",
          "Neutral",
          "slightly negative",
          "slightly negative",
          "Neutral",
          "Neutral",
          "Neutral",
          "Neutral",
          "Neutral",
          "Neutral",
          "very negative",
          "slightly positive",
          "slightly negative",
          "negative",
          "Neutral",
          "Neutral",
          "negative",
          "slightly negative",
          "Neutral",
          "Neutral",
          "slightly positive",
          "positive",
          "negative",
          "slightly negative",
          "slightly positive",
          "Neutral",
          "positive",
          "slightly negative",
          "very positive",
          "very positive",
          "Neutral",
          "negative",
          "slightly negative",
          "negative",
          "slightly negative",
          "positive",
          "very positive",
          "very positive",
          "very positive",
          "slightly positive",
          "slightly negative",
          "slightly positive",
          "positive",
          "slightly positive",
          "very negative",
          "very negative",
          "very positive",
          "very negative",
          "slightly negative",
          "very negative",
          "slightly positive",
          "slightly negative",
          "negative",
          "negative",
          "negative",
          "negative",
          "slightly positive",
          "slightly positive",
          "slightly positive",
          "negative",
          "positive",
          "very negative",
          "negative",
          "positive",
          "positive",
          "slightly positive",
          "Neutral",
          "slightly positive",
          "negative",
          "slightly negative",
          "Neutral",
          "slightly positive",
          "Neutral",
          "slightly negative",
          "Neutral",
          "Neutral",
          "Neutral",
          "Neutral",
          "slightly positive",
          "Neutral",
          "positive",
          "slightly negative",
          "positive",
          "slightly negative",
          "positive",
          "slightly negative",
          "Neutral",
          "very negative",
          "very negative",
          "slightly negative",
          "slightly negative",
          "negative",
          "very negative",
          "very positive",
          "very negative",
          "very negative",
          "negative",
          "negative",
          "very positive",
          "very negative",
          "very negative",
          "Neutral",
          "negative",
          "negative",
          "negative",
          "negative",
          "positive",
          "very positive",
          "very negative",
          "negative",
          "very negative",
          "very negative",
          "very negative",
          "slightly positive",
          "slightly negative",
          "slightly positive",
          "very negative",
          "negative",
          "very negative",
          "negative",
          "Neutral",
          "slightly negative",
          "very negative",
          "negative",
          "slightly positive",
          "Neutral",
          "negative",
          "slightly positive",
          "negative",
          "Neutral",
          "negative",
          "slightly negative",
          "positive",
          "very negative",
          "slightly negative",
          "Neutral",
          "negative",
          "slightly negative",
          "Neutral",
          "slightly negative",
          "positive",
          "positive",
          "slightly positive",
          "negative",
          "Neutral",
          "negative",
          "positive",
          "positive",
          "positive",
          "slightly positive",
          "slightly positive",
          "slightly negative",
          "negative",
          "negative",
          "Neutral",
          "slightly positive",
          "slightly positive",
          "negative",
          "slightly positive",
          "slightly positive",
          "very positive",
          "slightly positive",
          "slightly positive",
          "very negative",
          "negative",
          "slightly negative",
          "very negative",
          "negative",
          "negative",
          "slightly positive",
          "negative",
          "Neutral",
          "negative",
          "slightly positive",
          "negative",
          "negative",
          "negative",
          "negative",
          "negative",
          "positive",
          "very positive",
          "very positive",
          "slightly negative",
          "negative",
          "positive",
          "slightly negative",
          "very negative",
          "negative",
          "negative",
          "positive",
          "positive",
          "slightly positive",
          "slightly negative",
          "very negative",
          "negative",
          "positive",
          "slightly positive",
          "positive",
          "slightly negative",
          "slightly positive",
          "very negative",
          "very positive",
          "slightly negative",
          "very negative",
          "very negative",
          "very negative",
          "Neutral",
          "negative",
          "negative",
          "very negative",
          "very negative",
          "slightly negative",
          "negative",
          "very negative",
          "negative",
          "negative",
          "negative",
          "very negative",
          "negative",
          "slightly positive",
          "very negative",
          "very negative",
          "very positive",
          "very positive",
          "very positive",
          "positive",
          "Neutral",
          "slightly positive",
          "negative",
          "slightly negative",
          "slightly negative",
          "negative",
          "negative",
          "very negative",
          "negative",
          "very negative",
          "negative",
          "negative",
          "Neutral",
          "very negative",
          "Neutral",
          "negative",
          "slightly positive",
          "very negative",
          "very negative",
          "positive",
          "positive",
          "negative",
          "negative",
          "Neutral",
          "very negative",
          "slightly negative",
          "negative",
          "negative",
          "Neutral",
          "slightly positive",
          "Neutral",
          "negative",
          "negative",
          "slightly negative",
          "slightly negative",
          "negative",
          "slightly negative",
          "negative",
          "slightly negative",
          "negative",
          "slightly negative",
          "very negative",
          "slightly negative",
          "negative",
          "Neutral",
          "negative",
          "negative",
          "Neutral",
          "Neutral",
          "slightly negative",
          "very positive",
          "Neutral",
          "slightly positive",
          "Neutral",
          "positive",
          "slightly positive",
          "positive",
          "positive",
          "positive",
          "very positive",
          "positive",
          "Neutral",
          "positive",
          "positive",
          "very positive",
          "Neutral",
          "positive",
          "very positive",
          "positive",
          "very positive",
          "slightly positive",
          "slightly positive",
          "positive",
          "negative",
          "very positive",
          "negative",
          "positive",
          "Neutral",
          "Neutral",
          "very negative",
          "very positive",
          "positive",
          "positive",
          "positive",
          "very positive",
          "very positive",
          "Neutral",
          "positive",
          "Neutral",
          "Neutral",
          "very positive",
          "positive",
          "very positive",
          "Neutral",
          "positive",
          "very positive",
          "slightly positive",
          "very positive",
          "slightly positive",
          "slightly positive",
          "slightly positive",
          "slightly negative",
          "Neutral",
          "slightly positive",
          "Neutral",
          "positive",
          "slightly negative",
          "Neutral",
          "negative",
          "negative",
          "Neutral",
          "positive",
          "positive",
          "slightly negative",
          "positive",
          "Neutral",
          "positive",
          "slightly negative",
          "slightly negative",
          "Neutral",
          "positive",
          "positive",
          "positive",
          "positive",
          "positive",
          "positive",
          "Neutral",
          "very positive",
          "positive",
          "slightly positive",
          "slightly positive",
          "slightly positive",
          "slightly positive",
          "slightly negative",
          "slightly positive",
          "slightly negative",
          "Neutral",
          "negative",
          "slightly negative",
          "slightly negative",
          "negative",
          "slightly negative",
          "negative",
          "very positive",
          "slightly positive",
          "slightly positive",
          "Neutral",
          "negative",
          "negative",
          "slightly negative",
          "Neutral",
          "positive",
          "very positive",
          "slightly negative",
          "negative",
          "slightly negative",
          "negative",
          "Neutral",
          "Neutral",
          "Neutral",
          "negative",
          "Neutral",
          "negative",
          "negative",
          "positive",
          "slightly negative",
          "negative",
          "slightly negative",
          "negative",
          "negative",
          "slightly negative",
          "very negative",
          "negative",
          "slightly positive",
          "negative",
          "negative",
          "slightly negative",
          "negative",
          "positive",
          "negative",
          "negative",
          "very negative",
          "slightly positive",
          "slightly positive",
          "slightly negative",
          "slightly negative",
          "slightly positive",
          "very negative",
          "negative",
          "slightly negative",
          "negative",
          "negative",
          "positive",
          "positive",
          "slightly positive",
          "slightly negative",
          "negative",
          "slightly positive",
          "slightly positive",
          "slightly negative",
          "Neutral",
          "slightly positive",
          "Neutral",
          "slightly positive",
          "positive",
          "slightly negative",
          "negative",
          "slightly negative",
          "slightly positive",
          "Neutral",
          "slightly positive",
          "slightly positive"
         ],
         "xaxis": "x",
         "y": [
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "hovertemplate": "preds_7=%{x}<br>count=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "textposition": "auto",
         "type": "bar",
         "x": [
          "negative",
          "negative",
          "Neutral",
          "slightly positive",
          "negative",
          "negative",
          "negative",
          "slightly positive",
          "slightly positive",
          "slightly positive",
          "positive",
          "slightly negative",
          "slightly positive",
          "slightly positive",
          "negative",
          "slightly positive",
          "slightly positive",
          "negative",
          "positive",
          "negative",
          "slightly positive",
          "negative",
          "slightly negative",
          "negative",
          "slightly positive",
          "Neutral",
          "Neutral",
          "slightly positive",
          "slightly positive",
          "positive",
          "slightly negative",
          "slightly negative",
          "slightly negative",
          "Neutral",
          "slightly positive",
          "Neutral",
          "Neutral",
          "slightly negative",
          "very positive",
          "very positive",
          "Neutral",
          "slightly positive",
          "slightly positive",
          "positive",
          "slightly negative",
          "negative",
          "negative",
          "negative",
          "slightly negative",
          "slightly negative",
          "slightly positive",
          "Neutral",
          "very positive",
          "very positive",
          "positive",
          "positive",
          "positive",
          "negative",
          "positive",
          "negative",
          "slightly negative",
          "positive",
          "slightly negative",
          "negative",
          "positive",
          "very positive",
          "very positive",
          "slightly positive",
          "positive",
          "very positive",
          "Neutral",
          "very positive",
          "Neutral",
          "Neutral",
          "positive",
          "positive",
          "negative",
          "slightly positive",
          "slightly negative",
          "negative",
          "slightly positive",
          "positive",
          "very positive",
          "slightly positive",
          "very positive",
          "negative",
          "negative",
          "slightly negative",
          "negative",
          "slightly negative",
          "negative",
          "Neutral",
          "slightly negative",
          "slightly negative",
          "slightly negative",
          "slightly positive",
          "slightly negative",
          "Neutral",
          "slightly positive",
          "Neutral",
          "Neutral",
          "negative",
          "positive",
          "slightly negative",
          "positive",
          "slightly negative",
          "negative",
          "very positive",
          "Neutral",
          "very positive",
          "slightly positive",
          "Neutral",
          "very positive",
          "very positive",
          "positive",
          "slightly positive",
          "very positive",
          "very positive",
          "slightly positive",
          "Neutral",
          "positive",
          "negative",
          "negative",
          "slightly positive",
          "negative",
          "slightly positive",
          "slightly negative",
          "positive",
          "Neutral",
          "Neutral",
          "slightly negative",
          "negative",
          "positive",
          "negative",
          "negative",
          "Neutral",
          "negative",
          "negative",
          "negative",
          "negative",
          "slightly negative",
          "negative",
          "negative",
          "negative",
          "negative",
          "negative",
          "negative",
          "negative",
          "slightly negative",
          "slightly negative",
          "negative",
          "slightly negative",
          "slightly positive",
          "negative",
          "negative",
          "negative",
          "negative",
          "negative",
          "slightly negative",
          "negative",
          "negative",
          "negative",
          "negative",
          "positive",
          "slightly positive",
          "Neutral",
          "negative",
          "positive",
          "slightly positive",
          "slightly negative",
          "negative",
          "slightly positive",
          "negative",
          "positive",
          "slightly negative",
          "Neutral",
          "slightly positive",
          "positive",
          "positive",
          "slightly negative",
          "Neutral",
          "negative",
          "slightly positive",
          "slightly positive",
          "slightly positive",
          "negative",
          "slightly negative",
          "negative",
          "slightly negative",
          "negative",
          "slightly negative",
          "negative",
          "negative",
          "slightly negative",
          "negative",
          "negative",
          "slightly positive",
          "slightly negative",
          "slightly negative",
          "positive",
          "Neutral",
          "negative",
          "negative",
          "negative",
          "Neutral",
          "Neutral",
          "very negative",
          "negative",
          "negative",
          "slightly negative",
          "Neutral",
          "Neutral",
          "slightly negative",
          "positive",
          "slightly positive",
          "negative",
          "negative",
          "slightly positive",
          "positive",
          "positive",
          "positive",
          "slightly negative",
          "negative",
          "positive",
          "positive",
          "positive",
          "Neutral",
          "slightly negative",
          "positive",
          "slightly positive",
          "slightly negative",
          "Neutral",
          "slightly positive",
          "slightly positive",
          "negative",
          "slightly negative",
          "slightly negative",
          "Neutral",
          "negative",
          "Neutral",
          "slightly positive",
          "slightly negative",
          "slightly negative",
          "Neutral",
          "slightly negative",
          "slightly negative",
          "negative",
          "Neutral",
          "slightly negative",
          "negative",
          "negative",
          "Neutral",
          "Neutral",
          "Neutral",
          "slightly positive",
          "slightly negative",
          "Neutral",
          "slightly positive",
          "positive",
          "Neutral",
          "negative",
          "slightly negative",
          "positive",
          "negative",
          "slightly positive",
          "slightly positive",
          "Neutral",
          "slightly positive",
          "very positive",
          "slightly negative",
          "slightly negative",
          "positive",
          "Neutral",
          "very positive",
          "Neutral",
          "very positive",
          "very positive",
          "positive",
          "positive",
          "Neutral",
          "slightly negative",
          "slightly negative",
          "very positive",
          "very positive",
          "positive",
          "very positive",
          "slightly positive",
          "slightly negative",
          "slightly positive",
          "Neutral",
          "positive",
          "negative",
          "very negative",
          "negative",
          "negative",
          "slightly positive",
          "negative",
          "positive",
          "negative",
          "negative",
          "negative",
          "negative",
          "slightly negative",
          "slightly negative",
          "positive",
          "slightly positive",
          "negative",
          "positive",
          "negative",
          "Neutral",
          "positive",
          "slightly positive",
          "slightly positive",
          "slightly negative",
          "Neutral",
          "slightly negative",
          "positive",
          "negative",
          "positive",
          "slightly positive",
          "Neutral",
          "Neutral",
          "slightly negative",
          "positive",
          "positive",
          "positive",
          "slightly negative",
          "positive",
          "slightly positive",
          "positive",
          "Neutral",
          "positive",
          "negative",
          "slightly positive",
          "negative",
          "negative",
          "negative",
          "Neutral",
          "negative",
          "slightly negative",
          "very positive",
          "negative",
          "negative",
          "Neutral",
          "negative",
          "very positive",
          "negative",
          "negative",
          "negative",
          "slightly negative",
          "slightly negative",
          "negative",
          "negative",
          "Neutral",
          "very positive",
          "slightly negative",
          "negative",
          "negative",
          "negative",
          "negative",
          "positive",
          "slightly negative",
          "positive",
          "slightly positive",
          "slightly negative",
          "slightly negative",
          "Neutral",
          "slightly negative",
          "negative",
          "slightly negative",
          "slightly negative",
          "positive",
          "slightly positive",
          "slightly negative",
          "very positive",
          "negative",
          "slightly negative",
          "negative",
          "negative",
          "very positive",
          "negative",
          "slightly negative",
          "Neutral",
          "slightly negative",
          "Neutral",
          "Neutral",
          "slightly positive",
          "slightly positive",
          "positive",
          "very positive",
          "slightly positive",
          "slightly positive",
          "slightly negative",
          "very positive",
          "very positive",
          "positive",
          "positive",
          "positive",
          "slightly negative",
          "slightly negative",
          "slightly negative",
          "Neutral",
          "very positive",
          "very positive",
          "negative",
          "positive",
          "very positive",
          "very positive",
          "Neutral",
          "positive",
          "Neutral",
          "Neutral",
          "Neutral",
          "slightly negative",
          "slightly negative",
          "Neutral",
          "slightly positive",
          "negative",
          "Neutral",
          "slightly negative",
          "Neutral",
          "slightly positive",
          "slightly negative",
          "negative",
          "slightly negative",
          "Neutral",
          "very positive",
          "slightly positive",
          "very positive",
          "slightly negative",
          "negative",
          "slightly positive",
          "Neutral",
          "negative",
          "slightly positive",
          "negative",
          "positive",
          "slightly negative",
          "positive",
          "slightly negative",
          "Neutral",
          "slightly positive",
          "very positive",
          "Neutral",
          "positive",
          "positive",
          "positive",
          "negative",
          "positive",
          "Neutral",
          "negative",
          "slightly negative",
          "slightly negative",
          "Neutral",
          "negative",
          "negative",
          "negative",
          "slightly negative",
          "slightly positive",
          "negative",
          "negative",
          "negative",
          "slightly negative",
          "Neutral",
          "slightly negative",
          "slightly negative",
          "very positive",
          "negative",
          "slightly negative",
          "positive",
          "positive",
          "positive",
          "Neutral",
          "Neutral",
          "slightly negative",
          "slightly negative",
          "negative",
          "Neutral",
          "slightly negative",
          "Neutral",
          "negative",
          "Neutral",
          "negative",
          "positive",
          "slightly positive",
          "positive",
          "negative",
          "slightly negative",
          "negative",
          "positive",
          "negative",
          "negative",
          "positive",
          "positive",
          "Neutral",
          "Neutral",
          "slightly positive",
          "negative",
          "Neutral",
          "slightly positive",
          "slightly negative",
          "positive",
          "positive",
          "slightly positive",
          "negative",
          "negative",
          "negative",
          "negative",
          "slightly negative",
          "slightly negative",
          "negative",
          "slightly negative",
          "slightly positive",
          "Neutral",
          "slightly negative",
          "Neutral",
          "Neutral",
          "Neutral",
          "slightly negative",
          "slightly negative",
          "Neutral",
          "slightly positive",
          "slightly negative",
          "positive",
          "positive",
          "Neutral",
          "slightly negative",
          "positive",
          "positive",
          "slightly negative",
          "positive",
          "positive",
          "slightly positive",
          "positive",
          "slightly negative",
          "slightly positive",
          "positive",
          "positive",
          "slightly positive",
          "very positive",
          "very positive",
          "negative",
          "very positive",
          "very positive",
          "slightly negative",
          "slightly positive",
          "slightly negative",
          "very positive",
          "slightly negative",
          "slightly positive",
          "positive",
          "slightly positive",
          "slightly negative",
          "positive",
          "positive",
          "positive",
          "positive",
          "positive",
          "very positive",
          "Neutral",
          "slightly positive",
          "Neutral",
          "slightly positive",
          "positive",
          "positive",
          "positive",
          "slightly positive",
          "slightly positive",
          "very positive",
          "positive",
          "positive",
          "slightly positive",
          "Neutral",
          "slightly positive",
          "negative",
          "slightly positive",
          "negative",
          "very positive",
          "Neutral",
          "slightly negative",
          "slightly negative",
          "negative",
          "negative",
          "slightly negative",
          "positive",
          "positive",
          "slightly negative",
          "Neutral",
          "slightly negative",
          "positive",
          "Neutral",
          "slightly negative",
          "slightly positive",
          "slightly positive",
          "positive",
          "positive",
          "slightly positive",
          "slightly positive",
          "positive",
          "Neutral",
          "very positive",
          "very positive",
          "Neutral",
          "positive",
          "slightly negative",
          "negative",
          "Neutral",
          "Neutral",
          "negative",
          "Neutral",
          "Neutral",
          "positive",
          "Neutral",
          "Neutral",
          "negative",
          "slightly negative",
          "very positive",
          "slightly positive",
          "Neutral",
          "Neutral",
          "negative",
          "negative",
          "Neutral",
          "Neutral",
          "very positive",
          "positive",
          "slightly positive",
          "slightly negative",
          "negative",
          "negative",
          "slightly negative",
          "Neutral",
          "slightly negative",
          "slightly negative",
          "slightly negative",
          "slightly negative",
          "negative",
          "positive",
          "negative",
          "negative",
          "negative",
          "slightly negative",
          "negative",
          "slightly positive",
          "negative",
          "slightly negative",
          "slightly positive",
          "slightly negative",
          "Neutral",
          "slightly negative",
          "negative",
          "very positive",
          "negative",
          "negative",
          "negative",
          "positive",
          "slightly positive",
          "negative",
          "slightly negative",
          "slightly positive",
          "negative",
          "negative",
          "slightly negative",
          "negative",
          "negative",
          "positive",
          "positive",
          "positive",
          "slightly negative",
          "Neutral",
          "slightly positive",
          "slightly negative",
          "slightly negative",
          "slightly positive",
          "slightly positive",
          "Neutral",
          "Neutral",
          "slightly positive",
          "negative",
          "negative",
          "Neutral",
          "slightly positive",
          "slightly positive",
          "very positive",
          "positive"
         ],
         "xaxis": "x2",
         "y": [
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Gold",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "MAGBERT",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 600,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "CMU-MOSI 7 Class Sentiment Intensity"
        },
        "width": 1500,
        "xaxis": {
         "anchor": "y",
         "categoryarray": [
          "very negative",
          "negative",
          "slightly negative",
          "Neutral",
          "slightly positive",
          "positive",
          "very positive"
         ],
         "categoryorder": "array",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "categoryarray": [
          "very negative",
          "negative",
          "slightly negative",
          "Neutral",
          "slightly positive",
          "positive",
          "very positive"
         ],
         "categoryorder": "array",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "range": [
          0,
          200
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ],
         "range": [
          0,
          200
         ]
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"5474c8c6-ae8d-4e3c-a5ae-44c9b2944636\" class=\"plotly-graph-div\" style=\"height:600px; width:1500px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5474c8c6-ae8d-4e3c-a5ae-44c9b2944636\")) {                    Plotly.newPlot(                        \"5474c8c6-ae8d-4e3c-a5ae-44c9b2944636\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"labels_7=%{x}<br>count=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"very negative\",\"very negative\",\"slightly negative\",\"positive\",\"very negative\",\"very negative\",\"very negative\",\"very negative\",\"Neutral\",\"slightly positive\",\"slightly positive\",\"very negative\",\"very negative\",\"slightly positive\",\"very negative\",\"negative\",\"negative\",\"very negative\",\"negative\",\"negative\",\"slightly negative\",\"very negative\",\"slightly negative\",\"negative\",\"Neutral\",\"very negative\",\"negative\",\"slightly negative\",\"slightly positive\",\"slightly positive\",\"negative\",\"very negative\",\"negative\",\"very positive\",\"very positive\",\"slightly positive\",\"slightly positive\",\"very positive\",\"very positive\",\"very positive\",\"slightly negative\",\"Neutral\",\"positive\",\"positive\",\"slightly negative\",\"negative\",\"very negative\",\"very negative\",\"very negative\",\"very negative\",\"very positive\",\"slightly negative\",\"positive\",\"positive\",\"slightly positive\",\"slightly positive\",\"positive\",\"very negative\",\"slightly negative\",\"slightly negative\",\"very positive\",\"positive\",\"slightly negative\",\"very negative\",\"very positive\",\"very positive\",\"very positive\",\"Neutral\",\"very positive\",\"positive\",\"slightly negative\",\"positive\",\"Neutral\",\"slightly positive\",\"slightly positive\",\"positive\",\"slightly negative\",\"positive\",\"slightly negative\",\"slightly negative\",\"positive\",\"slightly positive\",\"positive\",\"positive\",\"positive\",\"slightly negative\",\"slightly negative\",\"Neutral\",\"slightly negative\",\"slightly negative\",\"Neutral\",\"slightly positive\",\"slightly negative\",\"slightly negative\",\"Neutral\",\"Neutral\",\"Neutral\",\"slightly negative\",\"slightly positive\",\"slightly positive\",\"very negative\",\"negative\",\"positive\",\"Neutral\",\"positive\",\"slightly negative\",\"very positive\",\"positive\",\"Neutral\",\"very positive\",\"positive\",\"Neutral\",\"very positive\",\"positive\",\"very positive\",\"positive\",\"very positive\",\"positive\",\"positive\",\"slightly positive\",\"Neutral\",\"very negative\",\"negative\",\"Neutral\",\"negative\",\"negative\",\"negative\",\"very negative\",\"very negative\",\"very negative\",\"slightly negative\",\"slightly negative\",\"positive\",\"negative\",\"negative\",\"negative\",\"negative\",\"negative\",\"negative\",\"very negative\",\"slightly positive\",\"very negative\",\"slightly negative\",\"very negative\",\"very negative\",\"negative\",\"very negative\",\"very negative\",\"slightly negative\",\"negative\",\"very negative\",\"negative\",\"slightly negative\",\"very negative\",\"very negative\",\"negative\",\"very negative\",\"very negative\",\"very negative\",\"very negative\",\"slightly negative\",\"very negative\",\"negative\",\"very negative\",\"Neutral\",\"negative\",\"slightly negative\",\"Neutral\",\"slightly positive\",\"slightly positive\",\"negative\",\"slightly positive\",\"slightly negative\",\"slightly positive\",\"slightly negative\",\"negative\",\"slightly negative\",\"negative\",\"slightly positive\",\"Neutral\",\"slightly positive\",\"negative\",\"slightly positive\",\"Neutral\",\"Neutral\",\"slightly negative\",\"slightly negative\",\"very negative\",\"negative\",\"negative\",\"slightly positive\",\"very negative\",\"slightly negative\",\"Neutral\",\"very negative\",\"Neutral\",\"positive\",\"slightly negative\",\"slightly negative\",\"slightly positive\",\"slightly positive\",\"very negative\",\"negative\",\"very negative\",\"slightly negative\",\"negative\",\"very negative\",\"negative\",\"slightly negative\",\"negative\",\"negative\",\"slightly positive\",\"slightly negative\",\"slightly negative\",\"negative\",\"slightly negative\",\"slightly negative\",\"Neutral\",\"positive\",\"positive\",\"positive\",\"very negative\",\"negative\",\"positive\",\"positive\",\"slightly positive\",\"negative\",\"slightly negative\",\"Neutral\",\"positive\",\"slightly positive\",\"slightly positive\",\"slightly positive\",\"positive\",\"Neutral\",\"slightly positive\",\"very negative\",\"slightly positive\",\"Neutral\",\"slightly negative\",\"positive\",\"slightly negative\",\"slightly negative\",\"Neutral\",\"very negative\",\"very negative\",\"very negative\",\"Neutral\",\"Neutral\",\"slightly negative\",\"slightly negative\",\"Neutral\",\"Neutral\",\"Neutral\",\"Neutral\",\"Neutral\",\"Neutral\",\"very negative\",\"slightly positive\",\"slightly negative\",\"negative\",\"Neutral\",\"Neutral\",\"negative\",\"slightly negative\",\"Neutral\",\"Neutral\",\"slightly positive\",\"positive\",\"negative\",\"slightly negative\",\"slightly positive\",\"Neutral\",\"positive\",\"slightly negative\",\"very positive\",\"very positive\",\"Neutral\",\"negative\",\"slightly negative\",\"negative\",\"slightly negative\",\"positive\",\"very positive\",\"very positive\",\"very positive\",\"slightly positive\",\"slightly negative\",\"slightly positive\",\"positive\",\"slightly positive\",\"very negative\",\"very negative\",\"very positive\",\"very negative\",\"slightly negative\",\"very negative\",\"slightly positive\",\"slightly negative\",\"negative\",\"negative\",\"negative\",\"negative\",\"slightly positive\",\"slightly positive\",\"slightly positive\",\"negative\",\"positive\",\"very negative\",\"negative\",\"positive\",\"positive\",\"slightly positive\",\"Neutral\",\"slightly positive\",\"negative\",\"slightly negative\",\"Neutral\",\"slightly positive\",\"Neutral\",\"slightly negative\",\"Neutral\",\"Neutral\",\"Neutral\",\"Neutral\",\"slightly positive\",\"Neutral\",\"positive\",\"slightly negative\",\"positive\",\"slightly negative\",\"positive\",\"slightly negative\",\"Neutral\",\"very negative\",\"very negative\",\"slightly negative\",\"slightly negative\",\"negative\",\"very negative\",\"very positive\",\"very negative\",\"very negative\",\"negative\",\"negative\",\"very positive\",\"very negative\",\"very negative\",\"Neutral\",\"negative\",\"negative\",\"negative\",\"negative\",\"positive\",\"very positive\",\"very negative\",\"negative\",\"very negative\",\"very negative\",\"very negative\",\"slightly positive\",\"slightly negative\",\"slightly positive\",\"very negative\",\"negative\",\"very negative\",\"negative\",\"Neutral\",\"slightly negative\",\"very negative\",\"negative\",\"slightly positive\",\"Neutral\",\"negative\",\"slightly positive\",\"negative\",\"Neutral\",\"negative\",\"slightly negative\",\"positive\",\"very negative\",\"slightly negative\",\"Neutral\",\"negative\",\"slightly negative\",\"Neutral\",\"slightly negative\",\"positive\",\"positive\",\"slightly positive\",\"negative\",\"Neutral\",\"negative\",\"positive\",\"positive\",\"positive\",\"slightly positive\",\"slightly positive\",\"slightly negative\",\"negative\",\"negative\",\"Neutral\",\"slightly positive\",\"slightly positive\",\"negative\",\"slightly positive\",\"slightly positive\",\"very positive\",\"slightly positive\",\"slightly positive\",\"very negative\",\"negative\",\"slightly negative\",\"very negative\",\"negative\",\"negative\",\"slightly positive\",\"negative\",\"Neutral\",\"negative\",\"slightly positive\",\"negative\",\"negative\",\"negative\",\"negative\",\"negative\",\"positive\",\"very positive\",\"very positive\",\"slightly negative\",\"negative\",\"positive\",\"slightly negative\",\"very negative\",\"negative\",\"negative\",\"positive\",\"positive\",\"slightly positive\",\"slightly negative\",\"very negative\",\"negative\",\"positive\",\"slightly positive\",\"positive\",\"slightly negative\",\"slightly positive\",\"very negative\",\"very positive\",\"slightly negative\",\"very negative\",\"very negative\",\"very negative\",\"Neutral\",\"negative\",\"negative\",\"very negative\",\"very negative\",\"slightly negative\",\"negative\",\"very negative\",\"negative\",\"negative\",\"negative\",\"very negative\",\"negative\",\"slightly positive\",\"very negative\",\"very negative\",\"very positive\",\"very positive\",\"very positive\",\"positive\",\"Neutral\",\"slightly positive\",\"negative\",\"slightly negative\",\"slightly negative\",\"negative\",\"negative\",\"very negative\",\"negative\",\"very negative\",\"negative\",\"negative\",\"Neutral\",\"very negative\",\"Neutral\",\"negative\",\"slightly positive\",\"very negative\",\"very negative\",\"positive\",\"positive\",\"negative\",\"negative\",\"Neutral\",\"very negative\",\"slightly negative\",\"negative\",\"negative\",\"Neutral\",\"slightly positive\",\"Neutral\",\"negative\",\"negative\",\"slightly negative\",\"slightly negative\",\"negative\",\"slightly negative\",\"negative\",\"slightly negative\",\"negative\",\"slightly negative\",\"very negative\",\"slightly negative\",\"negative\",\"Neutral\",\"negative\",\"negative\",\"Neutral\",\"Neutral\",\"slightly negative\",\"very positive\",\"Neutral\",\"slightly positive\",\"Neutral\",\"positive\",\"slightly positive\",\"positive\",\"positive\",\"positive\",\"very positive\",\"positive\",\"Neutral\",\"positive\",\"positive\",\"very positive\",\"Neutral\",\"positive\",\"very positive\",\"positive\",\"very positive\",\"slightly positive\",\"slightly positive\",\"positive\",\"negative\",\"very positive\",\"negative\",\"positive\",\"Neutral\",\"Neutral\",\"very negative\",\"very positive\",\"positive\",\"positive\",\"positive\",\"very positive\",\"very positive\",\"Neutral\",\"positive\",\"Neutral\",\"Neutral\",\"very positive\",\"positive\",\"very positive\",\"Neutral\",\"positive\",\"very positive\",\"slightly positive\",\"very positive\",\"slightly positive\",\"slightly positive\",\"slightly positive\",\"slightly negative\",\"Neutral\",\"slightly positive\",\"Neutral\",\"positive\",\"slightly negative\",\"Neutral\",\"negative\",\"negative\",\"Neutral\",\"positive\",\"positive\",\"slightly negative\",\"positive\",\"Neutral\",\"positive\",\"slightly negative\",\"slightly negative\",\"Neutral\",\"positive\",\"positive\",\"positive\",\"positive\",\"positive\",\"positive\",\"Neutral\",\"very positive\",\"positive\",\"slightly positive\",\"slightly positive\",\"slightly positive\",\"slightly positive\",\"slightly negative\",\"slightly positive\",\"slightly negative\",\"Neutral\",\"negative\",\"slightly negative\",\"slightly negative\",\"negative\",\"slightly negative\",\"negative\",\"very positive\",\"slightly positive\",\"slightly positive\",\"Neutral\",\"negative\",\"negative\",\"slightly negative\",\"Neutral\",\"positive\",\"very positive\",\"slightly negative\",\"negative\",\"slightly negative\",\"negative\",\"Neutral\",\"Neutral\",\"Neutral\",\"negative\",\"Neutral\",\"negative\",\"negative\",\"positive\",\"slightly negative\",\"negative\",\"slightly negative\",\"negative\",\"negative\",\"slightly negative\",\"very negative\",\"negative\",\"slightly positive\",\"negative\",\"negative\",\"slightly negative\",\"negative\",\"positive\",\"negative\",\"negative\",\"very negative\",\"slightly positive\",\"slightly positive\",\"slightly negative\",\"slightly negative\",\"slightly positive\",\"very negative\",\"negative\",\"slightly negative\",\"negative\",\"negative\",\"positive\",\"positive\",\"slightly positive\",\"slightly negative\",\"negative\",\"slightly positive\",\"slightly positive\",\"slightly negative\",\"Neutral\",\"slightly positive\",\"Neutral\",\"slightly positive\",\"positive\",\"slightly negative\",\"negative\",\"slightly negative\",\"slightly positive\",\"Neutral\",\"slightly positive\",\"slightly positive\"],\"xaxis\":\"x\",\"y\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"yaxis\":\"y\",\"type\":\"bar\"},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"preds_7=%{x}<br>count=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"negative\",\"negative\",\"Neutral\",\"slightly positive\",\"negative\",\"negative\",\"negative\",\"slightly positive\",\"slightly positive\",\"slightly positive\",\"positive\",\"slightly negative\",\"slightly positive\",\"slightly positive\",\"negative\",\"slightly positive\",\"slightly positive\",\"negative\",\"positive\",\"negative\",\"slightly positive\",\"negative\",\"slightly negative\",\"negative\",\"slightly positive\",\"Neutral\",\"Neutral\",\"slightly positive\",\"slightly positive\",\"positive\",\"slightly negative\",\"slightly negative\",\"slightly negative\",\"Neutral\",\"slightly positive\",\"Neutral\",\"Neutral\",\"slightly negative\",\"very positive\",\"very positive\",\"Neutral\",\"slightly positive\",\"slightly positive\",\"positive\",\"slightly negative\",\"negative\",\"negative\",\"negative\",\"slightly negative\",\"slightly negative\",\"slightly positive\",\"Neutral\",\"very positive\",\"very positive\",\"positive\",\"positive\",\"positive\",\"negative\",\"positive\",\"negative\",\"slightly negative\",\"positive\",\"slightly negative\",\"negative\",\"positive\",\"very positive\",\"very positive\",\"slightly positive\",\"positive\",\"very positive\",\"Neutral\",\"very positive\",\"Neutral\",\"Neutral\",\"positive\",\"positive\",\"negative\",\"slightly positive\",\"slightly negative\",\"negative\",\"slightly positive\",\"positive\",\"very positive\",\"slightly positive\",\"very positive\",\"negative\",\"negative\",\"slightly negative\",\"negative\",\"slightly negative\",\"negative\",\"Neutral\",\"slightly negative\",\"slightly negative\",\"slightly negative\",\"slightly positive\",\"slightly negative\",\"Neutral\",\"slightly positive\",\"Neutral\",\"Neutral\",\"negative\",\"positive\",\"slightly negative\",\"positive\",\"slightly negative\",\"negative\",\"very positive\",\"Neutral\",\"very positive\",\"slightly positive\",\"Neutral\",\"very positive\",\"very positive\",\"positive\",\"slightly positive\",\"very positive\",\"very positive\",\"slightly positive\",\"Neutral\",\"positive\",\"negative\",\"negative\",\"slightly positive\",\"negative\",\"slightly positive\",\"slightly negative\",\"positive\",\"Neutral\",\"Neutral\",\"slightly negative\",\"negative\",\"positive\",\"negative\",\"negative\",\"Neutral\",\"negative\",\"negative\",\"negative\",\"negative\",\"slightly negative\",\"negative\",\"negative\",\"negative\",\"negative\",\"negative\",\"negative\",\"negative\",\"slightly negative\",\"slightly negative\",\"negative\",\"slightly negative\",\"slightly positive\",\"negative\",\"negative\",\"negative\",\"negative\",\"negative\",\"slightly negative\",\"negative\",\"negative\",\"negative\",\"negative\",\"positive\",\"slightly positive\",\"Neutral\",\"negative\",\"positive\",\"slightly positive\",\"slightly negative\",\"negative\",\"slightly positive\",\"negative\",\"positive\",\"slightly negative\",\"Neutral\",\"slightly positive\",\"positive\",\"positive\",\"slightly negative\",\"Neutral\",\"negative\",\"slightly positive\",\"slightly positive\",\"slightly positive\",\"negative\",\"slightly negative\",\"negative\",\"slightly negative\",\"negative\",\"slightly negative\",\"negative\",\"negative\",\"slightly negative\",\"negative\",\"negative\",\"slightly positive\",\"slightly negative\",\"slightly negative\",\"positive\",\"Neutral\",\"negative\",\"negative\",\"negative\",\"Neutral\",\"Neutral\",\"very negative\",\"negative\",\"negative\",\"slightly negative\",\"Neutral\",\"Neutral\",\"slightly negative\",\"positive\",\"slightly positive\",\"negative\",\"negative\",\"slightly positive\",\"positive\",\"positive\",\"positive\",\"slightly negative\",\"negative\",\"positive\",\"positive\",\"positive\",\"Neutral\",\"slightly negative\",\"positive\",\"slightly positive\",\"slightly negative\",\"Neutral\",\"slightly positive\",\"slightly positive\",\"negative\",\"slightly negative\",\"slightly negative\",\"Neutral\",\"negative\",\"Neutral\",\"slightly positive\",\"slightly negative\",\"slightly negative\",\"Neutral\",\"slightly negative\",\"slightly negative\",\"negative\",\"Neutral\",\"slightly negative\",\"negative\",\"negative\",\"Neutral\",\"Neutral\",\"Neutral\",\"slightly positive\",\"slightly negative\",\"Neutral\",\"slightly positive\",\"positive\",\"Neutral\",\"negative\",\"slightly negative\",\"positive\",\"negative\",\"slightly positive\",\"slightly positive\",\"Neutral\",\"slightly positive\",\"very positive\",\"slightly negative\",\"slightly negative\",\"positive\",\"Neutral\",\"very positive\",\"Neutral\",\"very positive\",\"very positive\",\"positive\",\"positive\",\"Neutral\",\"slightly negative\",\"slightly negative\",\"very positive\",\"very positive\",\"positive\",\"very positive\",\"slightly positive\",\"slightly negative\",\"slightly positive\",\"Neutral\",\"positive\",\"negative\",\"very negative\",\"negative\",\"negative\",\"slightly positive\",\"negative\",\"positive\",\"negative\",\"negative\",\"negative\",\"negative\",\"slightly negative\",\"slightly negative\",\"positive\",\"slightly positive\",\"negative\",\"positive\",\"negative\",\"Neutral\",\"positive\",\"slightly positive\",\"slightly positive\",\"slightly negative\",\"Neutral\",\"slightly negative\",\"positive\",\"negative\",\"positive\",\"slightly positive\",\"Neutral\",\"Neutral\",\"slightly negative\",\"positive\",\"positive\",\"positive\",\"slightly negative\",\"positive\",\"slightly positive\",\"positive\",\"Neutral\",\"positive\",\"negative\",\"slightly positive\",\"negative\",\"negative\",\"negative\",\"Neutral\",\"negative\",\"slightly negative\",\"very positive\",\"negative\",\"negative\",\"Neutral\",\"negative\",\"very positive\",\"negative\",\"negative\",\"negative\",\"slightly negative\",\"slightly negative\",\"negative\",\"negative\",\"Neutral\",\"very positive\",\"slightly negative\",\"negative\",\"negative\",\"negative\",\"negative\",\"positive\",\"slightly negative\",\"positive\",\"slightly positive\",\"slightly negative\",\"slightly negative\",\"Neutral\",\"slightly negative\",\"negative\",\"slightly negative\",\"slightly negative\",\"positive\",\"slightly positive\",\"slightly negative\",\"very positive\",\"negative\",\"slightly negative\",\"negative\",\"negative\",\"very positive\",\"negative\",\"slightly negative\",\"Neutral\",\"slightly negative\",\"Neutral\",\"Neutral\",\"slightly positive\",\"slightly positive\",\"positive\",\"very positive\",\"slightly positive\",\"slightly positive\",\"slightly negative\",\"very positive\",\"very positive\",\"positive\",\"positive\",\"positive\",\"slightly negative\",\"slightly negative\",\"slightly negative\",\"Neutral\",\"very positive\",\"very positive\",\"negative\",\"positive\",\"very positive\",\"very positive\",\"Neutral\",\"positive\",\"Neutral\",\"Neutral\",\"Neutral\",\"slightly negative\",\"slightly negative\",\"Neutral\",\"slightly positive\",\"negative\",\"Neutral\",\"slightly negative\",\"Neutral\",\"slightly positive\",\"slightly negative\",\"negative\",\"slightly negative\",\"Neutral\",\"very positive\",\"slightly positive\",\"very positive\",\"slightly negative\",\"negative\",\"slightly positive\",\"Neutral\",\"negative\",\"slightly positive\",\"negative\",\"positive\",\"slightly negative\",\"positive\",\"slightly negative\",\"Neutral\",\"slightly positive\",\"very positive\",\"Neutral\",\"positive\",\"positive\",\"positive\",\"negative\",\"positive\",\"Neutral\",\"negative\",\"slightly negative\",\"slightly negative\",\"Neutral\",\"negative\",\"negative\",\"negative\",\"slightly negative\",\"slightly positive\",\"negative\",\"negative\",\"negative\",\"slightly negative\",\"Neutral\",\"slightly negative\",\"slightly negative\",\"very positive\",\"negative\",\"slightly negative\",\"positive\",\"positive\",\"positive\",\"Neutral\",\"Neutral\",\"slightly negative\",\"slightly negative\",\"negative\",\"Neutral\",\"slightly negative\",\"Neutral\",\"negative\",\"Neutral\",\"negative\",\"positive\",\"slightly positive\",\"positive\",\"negative\",\"slightly negative\",\"negative\",\"positive\",\"negative\",\"negative\",\"positive\",\"positive\",\"Neutral\",\"Neutral\",\"slightly positive\",\"negative\",\"Neutral\",\"slightly positive\",\"slightly negative\",\"positive\",\"positive\",\"slightly positive\",\"negative\",\"negative\",\"negative\",\"negative\",\"slightly negative\",\"slightly negative\",\"negative\",\"slightly negative\",\"slightly positive\",\"Neutral\",\"slightly negative\",\"Neutral\",\"Neutral\",\"Neutral\",\"slightly negative\",\"slightly negative\",\"Neutral\",\"slightly positive\",\"slightly negative\",\"positive\",\"positive\",\"Neutral\",\"slightly negative\",\"positive\",\"positive\",\"slightly negative\",\"positive\",\"positive\",\"slightly positive\",\"positive\",\"slightly negative\",\"slightly positive\",\"positive\",\"positive\",\"slightly positive\",\"very positive\",\"very positive\",\"negative\",\"very positive\",\"very positive\",\"slightly negative\",\"slightly positive\",\"slightly negative\",\"very positive\",\"slightly negative\",\"slightly positive\",\"positive\",\"slightly positive\",\"slightly negative\",\"positive\",\"positive\",\"positive\",\"positive\",\"positive\",\"very positive\",\"Neutral\",\"slightly positive\",\"Neutral\",\"slightly positive\",\"positive\",\"positive\",\"positive\",\"slightly positive\",\"slightly positive\",\"very positive\",\"positive\",\"positive\",\"slightly positive\",\"Neutral\",\"slightly positive\",\"negative\",\"slightly positive\",\"negative\",\"very positive\",\"Neutral\",\"slightly negative\",\"slightly negative\",\"negative\",\"negative\",\"slightly negative\",\"positive\",\"positive\",\"slightly negative\",\"Neutral\",\"slightly negative\",\"positive\",\"Neutral\",\"slightly negative\",\"slightly positive\",\"slightly positive\",\"positive\",\"positive\",\"slightly positive\",\"slightly positive\",\"positive\",\"Neutral\",\"very positive\",\"very positive\",\"Neutral\",\"positive\",\"slightly negative\",\"negative\",\"Neutral\",\"Neutral\",\"negative\",\"Neutral\",\"Neutral\",\"positive\",\"Neutral\",\"Neutral\",\"negative\",\"slightly negative\",\"very positive\",\"slightly positive\",\"Neutral\",\"Neutral\",\"negative\",\"negative\",\"Neutral\",\"Neutral\",\"very positive\",\"positive\",\"slightly positive\",\"slightly negative\",\"negative\",\"negative\",\"slightly negative\",\"Neutral\",\"slightly negative\",\"slightly negative\",\"slightly negative\",\"slightly negative\",\"negative\",\"positive\",\"negative\",\"negative\",\"negative\",\"slightly negative\",\"negative\",\"slightly positive\",\"negative\",\"slightly negative\",\"slightly positive\",\"slightly negative\",\"Neutral\",\"slightly negative\",\"negative\",\"very positive\",\"negative\",\"negative\",\"negative\",\"positive\",\"slightly positive\",\"negative\",\"slightly negative\",\"slightly positive\",\"negative\",\"negative\",\"slightly negative\",\"negative\",\"negative\",\"positive\",\"positive\",\"positive\",\"slightly negative\",\"Neutral\",\"slightly positive\",\"slightly negative\",\"slightly negative\",\"slightly positive\",\"slightly positive\",\"Neutral\",\"Neutral\",\"slightly positive\",\"negative\",\"negative\",\"Neutral\",\"slightly positive\",\"slightly positive\",\"very positive\",\"positive\"],\"xaxis\":\"x2\",\"y\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"yaxis\":\"y2\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"categoryorder\":\"array\",\"categoryarray\":[\"very negative\",\"negative\",\"slightly negative\",\"Neutral\",\"slightly positive\",\"positive\",\"very positive\"]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"range\":[0,200]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"categoryorder\":\"array\",\"categoryarray\":[\"very negative\",\"negative\",\"slightly negative\",\"Neutral\",\"slightly positive\",\"positive\",\"very positive\"]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"range\":[0,200]},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Gold\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"MAGBERT\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"CMU-MOSI 7 Class Sentiment Intensity\"},\"height\":600,\"width\":1500},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('5474c8c6-ae8d-4e3c-a5ae-44c9b2944636');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import pandas as pd\n",
    "\n",
    "d = {'segmentID': segment_list, 'words': words_list, 'labels': labels, 'labels_2': labels_2, 'labels_7': labels_7, 'preds': preds, 'preds_2': preds_2, 'preds_7': preds_7}\n",
    "df = pd.DataFrame(data=d)\n",
    "order = ['very negative', 'negative', 'slightly negative', 'Neutral', 'slightly positive', 'positive', 'very positive']\n",
    "\n",
    "fig1 = px.bar(df, x=\"labels_7\")\n",
    "fig2 = px.bar(df, x=\"preds_7\")\n",
    "\n",
    "fig1_traces = []\n",
    "fig2_traces = []\n",
    "\n",
    "for trace in range(len(fig1[\"data\"])):\n",
    "    fig1_traces.append(fig1[\"data\"][trace])\n",
    "for trace in range(len(fig2[\"data\"])):\n",
    "    fig2_traces.append(fig2[\"data\"][trace])\n",
    "\n",
    "this_figure = sp.make_subplots(rows=1, cols=2, subplot_titles=(\"Gold\", \"MAGBERT\"))\n",
    "for traces in fig1_traces:\n",
    "    this_figure.append_trace(traces, row=1, col=1)\n",
    "for traces in fig2_traces:\n",
    "    this_figure.append_trace(traces, row=1, col=2)\n",
    "\n",
    "this_figure.update_layout(height=600, width=1500, title_text=\"CMU-MOSI 7 Class Sentiment Intensity\")\n",
    "this_figure.update_xaxes(categoryorder='array', categoryarray= order)\n",
    "this_figure.update_yaxes(range=[0,200])\n",
    "this_figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99b221e9db0318903f19c34b1ba0eab49364574753058918f60cea91a58935ca"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('pytorch1.7.1_p37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
